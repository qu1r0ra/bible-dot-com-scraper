{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697467f1",
   "metadata": {},
   "source": [
    "# Parallel Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecea2a0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3245aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Iterable, Union\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d8fe4",
   "metadata": {},
   "source": [
    "## By verse\n",
    "This script provides a reusable pipeline for building parallel corpora of Bible translations by verse across multiple language pairs. Each pair is configured with input directories containing verse-aligned .txt files for two languages and an optional output filename. \n",
    "\n",
    "The program parses these files into dictionaries keyed by USFM identifiers (e.g., 1CO.5.12), applies a configurable de-duplication strategy (first, last, or join with a separator), and merges all verses within a folder. For each language pair, the script aligns verses across both languages, fills in missing entries with a customizable placeholder token, and writes an interleaved output file where verses appear in matched pairs. Output files are automatically named based on the language directories if not explicitly specified. \n",
    "\n",
    "A summary is produced for each pair, including counts of total verses, lines written, and missing verses per language. This design supports flexible scaling: additional language pairs can be added to the PAIRS list with minimal configuration, while defaults for deduplication, missing values, and join behavior ensure consistency across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95c56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_verse.txt\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_verse.txt\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/cj/parsed/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_verse.txt\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_verse.txt\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddaa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Kapampangan True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Bikolano True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Tausug True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Yami True\n",
      "---\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Ilokano True\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8e16520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults / helpers\n",
    "\n",
    "DEFAULT_DEDUP_STRATEGY = \"first\"  \n",
    "DEFAULT_MISSING = '\"N/A\"'\n",
    "DEFAULT_JOIN_SEP = \" | \"\n",
    "\n",
    "LINE_RE = re.compile(r'^\\s*([0-9A-Z]+\\.\\d+\\.\\d+)\\s+(.*\\S)\\s*$')\n",
    "\n",
    "def _to_path(p: Union[str, Path, None]) -> Union[Path, None]:\n",
    "    return None if p is None else (p if isinstance(p, Path) else Path(p))\n",
    "\n",
    "def parse_txt_file(fp: Path, dedup_strategy: str, join_sep: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse a single .txt file into {usfm_id: text}, deduping within the file.\n",
    "    \"\"\"\n",
    "    d: Dict[str, str] = {}\n",
    "    with fp.open(\"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "        for raw in f:\n",
    "            m = LINE_RE.match(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            usfm_id, text = m.group(1), m.group(2).strip()\n",
    "            if usfm_id in d:\n",
    "                if dedup_strategy == \"join\" and text not in d[usfm_id]:\n",
    "                    d[usfm_id] = d[usfm_id] + join_sep + text\n",
    "                elif dedup_strategy == \"last\":\n",
    "                    d[usfm_id] = text\n",
    "            else:\n",
    "                d[usfm_id] = text\n",
    "    return d\n",
    "\n",
    "def load_folder(folder: Path, dedup_strategy: str, join_sep: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Merge all *.txt files in a folder into a single map {usfm_id: text}.\n",
    "    \"\"\"\n",
    "    combined: Dict[str, str] = {}\n",
    "    txts = sorted(folder.glob(\"*.txt\"))\n",
    "    for fp in txts:\n",
    "        part = parse_txt_file(fp, dedup_strategy, join_sep)\n",
    "        for k, v in part.items():\n",
    "            if k in combined:\n",
    "                if dedup_strategy == \"join\" and v not in combined[k]:\n",
    "                    combined[k] = combined[k] + join_sep + v\n",
    "                elif dedup_strategy == \"last\":\n",
    "                    combined[k] = v\n",
    "            else:\n",
    "                combined[k] = v\n",
    "    return combined\n",
    "\n",
    "def usfm_sort_key(usfm: str):\n",
    "    \"\"\"\n",
    "    Sort key: (book_code, chapter:int, verse:int).\n",
    "    Note: book order is lexical by code; adjust if you want canonical order.\n",
    "    \"\"\"\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if parts else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return (book, chap, verse)\n",
    "\n",
    "def split_usfm(usfm: str):\n",
    "    \"\"\"\n",
    "    Return (book, chapter:int, verse:int) from a USFM like '1CO.5.11'.\n",
    "    \"\"\"\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if len(parts) > 0 else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return book, chap, verse\n",
    "\n",
    "def _last_dir_name(p: Path) -> str:\n",
    "    return p.name or p.parent.name\n",
    "\n",
    "def process_pair(\n",
    "    lang1_dir: Path,\n",
    "    lang2_dir: Path,\n",
    "    out_txt: Path,\n",
    "    dedup_strategy: str = DEFAULT_DEDUP_STRATEGY,\n",
    "    missing: str = DEFAULT_MISSING,\n",
    "    join_sep: str = DEFAULT_JOIN_SEP,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process one language pair and write:\n",
    "      - TXT with stacked lines per USFM:  \"<USFM> <lang1_text>\" then \"<USFM> <lang2_text>\"\n",
    "      - CSV with columns: usfm, book, verse, chapter, language 1, language2\n",
    "    Returns a small summary dict.\n",
    "    \"\"\"\n",
    "    assert lang1_dir.is_dir(), f\"Not a directory: {lang1_dir}\"\n",
    "    assert lang2_dir.is_dir(), f\"Not a directory: {lang2_dir}\"\n",
    "\n",
    "    if out_txt is None:\n",
    "        name1 = _last_dir_name(lang1_dir)\n",
    "        name2 = _last_dir_name(lang2_dir)\n",
    "        out_txt = Path(f\"{name1}_{name2}_verse.txt\")\n",
    "    out_csv = out_txt.with_suffix(\".csv\")\n",
    "\n",
    "    # Load data\n",
    "    lang1 = load_folder(lang1_dir, dedup_strategy, join_sep)\n",
    "    lang2 = load_folder(lang2_dir, dedup_strategy, join_sep)\n",
    "\n",
    "    all_usfm = sorted(set(lang1.keys()) | set(lang2.keys()), key=usfm_sort_key)\n",
    "\n",
    "    # Write TXT\n",
    "    lines_out = []\n",
    "    for u in all_usfm:\n",
    "        t1 = lang1.get(u, missing)\n",
    "        t2 = lang2.get(u, missing)\n",
    "        lines_out.append(f\"{u} {t1}\")\n",
    "        lines_out.append(f\"{u} {t2}\")\n",
    "    out_txt.write_text(\"\\n\".join(lines_out) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    # Write CSV\n",
    "    with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"usfm\", \"book\", \"verse\", \"chapter\", \"language 1\", \"language2\"])\n",
    "        for u in all_usfm:\n",
    "            t1 = lang1.get(u, missing)\n",
    "            t2 = lang2.get(u, missing)\n",
    "            book, chap, ver = split_usfm(u)\n",
    "            w.writerow([u, book, ver, chap, t1, t2])\n",
    "\n",
    "    missing_in_lang1 = sum(1 for u in all_usfm if u not in lang1)\n",
    "    missing_in_lang2 = sum(1 for u in all_usfm if u not in lang2)\n",
    "\n",
    "    summary = {\n",
    "        \"out_txt\": str(out_txt),\n",
    "        \"out_csv\": str(out_csv),\n",
    "        \"txt_lines_written\": len(lines_out),\n",
    "        \"csv_rows_written\": len(all_usfm) + 1,  # +1 for header\n",
    "        \"verses_total\": len(all_usfm),\n",
    "        \"missing_in_lang1\": missing_in_lang1,\n",
    "        \"missing_in_lang2\": missing_in_lang2,\n",
    "        \"dedup_strategy\": dedup_strategy,\n",
    "        \"join_sep\": join_sep,\n",
    "        \"missing_token\": missing,\n",
    "    }\n",
    "    print(\n",
    "        f\"Processed {out_txt.name} & {out_csv.name} | verses: {summary['verses_total']} | \"\n",
    "        f\"missing(lang1): {summary['missing_in_lang1']} | missing(lang2): {summary['missing_in_lang2']} | \"\n",
    "        f\"dedup={dedup_strategy}\"\n",
    "    )\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e3c6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed tagalog_kapampangan_verse.txt & tagalog_kapampangan_verse.csv | verses: 35379 | missing(lang1): 1 | missing(lang2): 27 | dedup=first\n",
      "Processed tagalog_bikolano_verse.txt & tagalog_bikolano_verse.csv | verses: 35379 | missing(lang1): 1 | missing(lang2): 36 | dedup=first\n",
      "Processed cebuano_spanish_verse.txt & cebuano_spanish_verse.csv | verses: 31105 | missing(lang1): 17 | missing(lang2): 1 | dedup=first\n",
      "Processed cebuano_tausug_verse.txt & cebuano_tausug_verse.csv | verses: 31104 | missing(lang1): 16 | missing(lang2): 23145 | dedup=first\n",
      "Processed chavacano_spanish_verse.txt & chavacano_spanish_verse.csv | verses: 31105 | missing(lang1): 23146 | missing(lang2): 1 | dedup=first\n",
      "Processed ivatan_yami_verse.txt & ivatan_yami_verse.csv | verses: 9566 | missing(lang1): 1 | missing(lang2): 1607 | dedup=first\n",
      "Processed pangasinense_ilokano_verse.txt & pangasinense_ilokano_verse.csv | verses: 35326 | missing(lang1): 5 | missing(lang2): 4243 | dedup=first\n",
      "\n",
      "Done processing all pairs.\n",
      "- Wrote TXT: tagalog_kapampangan_verse.txt | CSV: tagalog_kapampangan_verse.csv\n",
      "- Wrote TXT: tagalog_bikolano_verse.txt | CSV: tagalog_bikolano_verse.csv\n",
      "- Wrote TXT: cebuano_spanish_verse.txt | CSV: cebuano_spanish_verse.csv\n",
      "- Wrote TXT: cebuano_tausug_verse.txt | CSV: cebuano_tausug_verse.csv\n",
      "- Wrote TXT: chavacano_spanish_verse.txt | CSV: chavacano_spanish_verse.csv\n",
      "- Wrote TXT: ivatan_yami_verse.txt | CSV: ivatan_yami_verse.csv\n",
      "- Wrote TXT: pangasinense_ilokano_verse.txt | CSV: pangasinense_ilokano_verse.csv\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "all_summaries = []\n",
    "for cfg in PAIRS:\n",
    "    lang1_dir = _to_path(cfg.get(\"lang1_dir\"))\n",
    "    lang2_dir = _to_path(cfg.get(\"lang2_dir\"))\n",
    "    out_txt   = _to_path(cfg.get(\"out_txt\"))\n",
    "\n",
    "    dedup_strategy = cfg.get(\"dedup_strategy\", DEFAULT_DEDUP_STRATEGY)\n",
    "    missing        = cfg.get(\"missing\", DEFAULT_MISSING)\n",
    "    join_sep       = cfg.get(\"join_sep\", DEFAULT_JOIN_SEP)\n",
    "\n",
    "    if lang1_dir is None or lang2_dir is None:\n",
    "        raise ValueError(\"Each pair must include 'lang1_dir' and 'lang2_dir'.\")\n",
    "\n",
    "    summary = process_pair(\n",
    "        lang1_dir=lang1_dir,\n",
    "        lang2_dir=lang2_dir,\n",
    "        out_txt=out_txt,\n",
    "        dedup_strategy=dedup_strategy,\n",
    "        missing=missing,\n",
    "        join_sep=join_sep,\n",
    "    )\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nDone processing all pairs.\")\n",
    "for s in all_summaries:\n",
    "    print(f\"- Wrote TXT: {s['out_txt']} | CSV: {s['out_csv']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e76df",
   "metadata": {},
   "source": [
    "## By sentence\n",
    "\n",
    "This code builds parallel corpora for multiple language pairs by aligning sentence-level CSV files book by book. The PAIRS list defines which languages to pair, with input directories and output filenames. Helper functions handle reading CSVs, normalizing columns, detecting missing values, and natural sorting of files. \n",
    "\n",
    "The script matches books between two languages by first building a map of all available books in each directory, using the book code extracted from filenames (e.g., MAT, 1CO). It then takes the union of both sets of book codes so every book that appears in either language is included. If a book exists in one language but not the other, the missing side is padded with placeholder entries (N/A). This ensures that all books are represented, with sentences aligned where both languages exist and gaps clearly marked when they do not.\n",
    "\n",
    "Each book’s sentences are extracted, aligned with padding (N/A if missing), and combined into a DataFrame. The results are written both as CSV (for structured analysis) and TXT (for human-readable format). Along the way, the script logs statistics on books, rows, and missing entries. In short, it automates the process of matching bilingual sentence data into clean, aligned outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd694bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_sentence\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_sentence\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/cj/sentence/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_sentence\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_sentence\",\n",
    "    },\n",
    "]\n",
    "\n",
    "OUT_DIR = None\n",
    "USE_FIXED_HEADERS = False\n",
    "KEEP_EMPTY_LINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b80bccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Kapampangan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Bikolano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Tausug True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Yami True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Ilokano True\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9e1bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books (total): 77; Common books: 77\n",
      "Total rows (after padding with N/A): 53139\n",
      "CSV → tagalog_kapampangan_sentence.csv\n",
      "TXT → tagalog_kapampangan_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1130, 'Kapampangan_rows': 1257, 'aligned_pairs': 1257}\n",
      "{'book': '1CO', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 606, 'Kapampangan_rows': 609, 'aligned_pairs': 609}\n",
      "{'book': '1JN', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 156, 'Kapampangan_rows': 159, 'aligned_pairs': 159}\n",
      "{'book': '1KI', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1361, 'Kapampangan_rows': 1263, 'aligned_pairs': 1361}\n",
      "{'book': '1MA', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1322, 'Kapampangan_rows': 1208, 'aligned_pairs': 1322}\n",
      "{'book': '1PE', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 154, 'Kapampangan_rows': 156, 'aligned_pairs': 156}\n",
      "[1] [1CH] Tagalog(MBB05_1CH_raw#1): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Kapampangan(PmPV_1CH_raw#1): Anak neng Adan i Set, at anak neng Set i Enos.\n",
      "[2] [1CH] Tagalog(MBB05_1CH_raw#2): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Kapampangan(PmPV_1CH_raw#2): Anak neng Enos i Kenan, at anak neng Kenan i Mahalalel.\n",
      "[3] [1CH] Tagalog(MBB05_1CH_raw#3): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Kapampangan(PmPV_1CH_raw#3): Anak neng Mahalalel i Jared.\n",
      "Books (total): 77; Common books: 77\n",
      "Total rows (after padding with N/A): 51856\n",
      "CSV → tagalog_bikolano_sentence.csv\n",
      "TXT → tagalog_bikolano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1130, 'Bikolano_rows': 980, 'aligned_pairs': 1130}\n",
      "{'book': '1CO', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 606, 'Bikolano_rows': 608, 'aligned_pairs': 608}\n",
      "{'book': '1JN', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 156, 'Bikolano_rows': 143, 'aligned_pairs': 156}\n",
      "{'book': '1KI', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1361, 'Bikolano_rows': 1180, 'aligned_pairs': 1361}\n",
      "{'book': '1MA', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1322, 'Bikolano_rows': 1168, 'aligned_pairs': 1322}\n",
      "{'book': '1PE', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 154, 'Bikolano_rows': 139, 'aligned_pairs': 154}\n",
      "[1] [1CH] Tagalog(MBB05_1CH_raw#1): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Bikolano(MBBBIK92_1CH_raw#1): Si Adan iyo an ama ni Set, asin si Set iyo an ama ni Enos na ama ni Kenan.\n",
      "[2] [1CH] Tagalog(MBB05_1CH_raw#2): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Bikolano(MBBBIK92_1CH_raw#2): Si Kenan iyo an ama ni Mahalalel na ama ni Jared.\n",
      "[3] [1CH] Tagalog(MBB05_1CH_raw#3): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Bikolano(MBBBIK92_1CH_raw#3): Si Jared iyo an ama ni Enoc na ama ni Metusela; si Metusela iyo an ama ni Lamec na iyo an ama ni Noe.\n",
      "Books (total): 66; Common books: 66\n",
      "Total rows (after padding with N/A): 35777\n",
      "CSV → cebuano_spanish_sentence.csv\n",
      "TXT → cebuano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 880, 'Spanish_rows': 845, 'aligned_pairs': 880}\n",
      "{'book': '1CO', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 522, 'Spanish_rows': 446, 'aligned_pairs': 522}\n",
      "{'book': '1JN', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 123, 'Spanish_rows': 123, 'aligned_pairs': 123}\n",
      "{'book': '1KI', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 963, 'Spanish_rows': 980, 'aligned_pairs': 980}\n",
      "{'book': '1PE', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 103, 'Spanish_rows': 87, 'aligned_pairs': 103}\n",
      "{'book': '1SA', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 1188, 'Spanish_rows': 1144, 'aligned_pairs': 1188}\n",
      "[1] [1CH] Cebuano(ABCEB_1CH_raw#1): Si Adan, si Set, si Enos, si Kenan, si Mahalalel, si Jared, si Enoc, si Metusela, si Lamec, si Noe, si Sem, si Ham ug si Jafet.  ||  Spanish(RVR95_1CH_raw#1): Adán, Set, Enós, Cainán, Mahalaleel, Jared, Enoc, Matusalén, Lamec, Noé, Sem, Cam y Jafet.\n",
      "[2] [1CH] Cebuano(ABCEB_1CH_raw#2): Ang mga anak nga lalaki ni Jafet: si Gomer, si Magog, si Madai, si Javan, si Tubal, si Mesec ug si Tiras.  ||  Spanish(RVR95_1CH_raw#2): Los hijos de Jafet: Gomer, Magog, Madai, Javán, Tubal, Mesec y Tiras.\n",
      "[3] [1CH] Cebuano(ABCEB_1CH_raw#3): Ang mga anak nga lalaki ni Gomer: si Askenas, si Rifat ug si Togarma.  ||  Spanish(RVR95_1CH_raw#3): Los hijos de Gomer: Askenaz, Rifat y Togarma.\n",
      "Books (total): 66; Common books: 27\n",
      "Books only in Cebuano: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total rows (after padding with N/A): 42448\n",
      "CSV → cebuano_tausug_sentence.csv\n",
      "TXT → cebuano_tausug_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 880, 'Tausug_rows': 0, 'aligned_pairs': 880}\n",
      "{'book': '1CO', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 522, 'Tausug_rows': 901, 'aligned_pairs': 901}\n",
      "{'book': '1JN', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 123, 'Tausug_rows': 187, 'aligned_pairs': 187}\n",
      "{'book': '1KI', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 963, 'Tausug_rows': 0, 'aligned_pairs': 963}\n",
      "{'book': '1PE', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 103, 'Tausug_rows': 235, 'aligned_pairs': 235}\n",
      "{'book': '1SA', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 1188, 'Tausug_rows': 0, 'aligned_pairs': 1188}\n",
      "[1] [1CH] Cebuano(ABCEB_1CH_raw#1): Si Adan, si Set, si Enos, si Kenan, si Mahalalel, si Jared, si Enoc, si Metusela, si Lamec, si Noe, si Sem, si Ham ug si Jafet.  ||  Tausug(N/A): N/A\n",
      "[2] [1CH] Cebuano(ABCEB_1CH_raw#2): Ang mga anak nga lalaki ni Jafet: si Gomer, si Magog, si Madai, si Javan, si Tubal, si Mesec ug si Tiras.  ||  Tausug(N/A): N/A\n",
      "[3] [1CH] Cebuano(ABCEB_1CH_raw#3): Ang mga anak nga lalaki ni Gomer: si Askenas, si Rifat ug si Togarma.  ||  Tausug(N/A): N/A\n",
      "Books (total): 66; Common books: 27\n",
      "Books only in Spanish: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total rows (after padding with N/A): 37964\n",
      "CSV → chavacano_spanish_sentence.csv\n",
      "TXT → chavacano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 845, 'aligned_pairs': 845}\n",
      "{'book': '1CO', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 749, 'Spanish_rows': 446, 'aligned_pairs': 749}\n",
      "{'book': '1JN', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 154, 'Spanish_rows': 123, 'aligned_pairs': 154}\n",
      "{'book': '1KI', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 980, 'aligned_pairs': 980}\n",
      "{'book': '1PE', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 187, 'Spanish_rows': 87, 'aligned_pairs': 187}\n",
      "{'book': '1SA', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 1144, 'aligned_pairs': 1144}\n",
      "[1] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#1): Adán, Set, Enós, Cainán, Mahalaleel, Jared, Enoc, Matusalén, Lamec, Noé, Sem, Cam y Jafet.\n",
      "[2] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#2): Los hijos de Jafet: Gomer, Magog, Madai, Javán, Tubal, Mesec y Tiras.\n",
      "[3] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#3): Los hijos de Gomer: Askenaz, Rifat y Togarma.\n",
      "Books (total): 29; Common books: 27\n",
      "Books only in Ivatan: ['PRO', 'PSA']\n",
      "Total rows (after padding with N/A): 18175\n",
      "CSV → ivatan_yami_sentence.csv\n",
      "TXT → ivatan_yami_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CO', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 500, 'Yami_rows': 944, 'aligned_pairs': 944}\n",
      "{'book': '1JN', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 112, 'Yami_rows': 236, 'aligned_pairs': 236}\n",
      "{'book': '1PE', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 111, 'Yami_rows': 259, 'aligned_pairs': 259}\n",
      "{'book': '1TH', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 63, 'Yami_rows': 173, 'aligned_pairs': 173}\n",
      "{'book': '1TI', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 126, 'Yami_rows': 247, 'aligned_pairs': 247}\n",
      "{'book': '2CO', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 273, 'Yami_rows': 612, 'aligned_pairs': 612}\n",
      "[1] [1CO] Ivatan(VTSP_1CO_raw#1): Si Pablo ako a tinawagan no Dios a asa ka apostol ni Jesu Cristo a makayamot do inolay na as kani Sostenes a kakteh ta.  ||  Yami(SNT_1CO_raw#1): Yaken rana am si Pawlo ko a yana nipamnekan ni Yeso Kizisto a amlivolivon no ciriciring na a lamlamsoy na ni Ama ta do to.\n",
      "[2] [1CO] Ivatan(VTSP_1CO_raw#2): Tayto kami a maytolas dinio do timban aya no Dios do Corinto, dinio a pinayvadiw na a manamonamo a makayamot di Jesu Cristo as tinawagan na a tawotawo na a akma sira so kadwan do matatarek a logar a tayto a omanianib so Apo taya a si Jesu Cristo a iya so Apohen ta atavo.  ||  Yami(SNT_1CO_raw#2): Yaken rana kano akma tey kakteh do cinai si Sotenay am namen ipeyteygami inyo a sinjya do Kedinto a kyokay.\n",
      "[3] [1CO] Ivatan(VTSP_1CO_raw#3): Bendisionan kamo pa no Dios Ama as kano Apo taya a si Jesu Cristo as kapakarawat nio pa so grasia kano kaydamnayan no aktokto.  ||  Yami(SNT_1CO_raw#3): Inyo rana kano yamakakaday do peycyeylilyan na a macikaop ji Kizisto a tao am yatamo meyyangangay a na nipamnekan ni Ama ta do to a italamozong na.\n",
      "Books (total): 77; Common books: 66\n",
      "Books only in Pangasinense: ['1MA', '2MA', 'BAR', 'BEL', 'JDT', 'LJE', 'S3Y', 'SIR', 'SUS', 'TOB']...\n",
      "Total rows (after padding with N/A): 55824\n",
      "CSV → pangasinense_ilokano_sentence.csv\n",
      "TXT → pangasinense_ilokano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 1076, 'Ilokano_rows': 1153, 'aligned_pairs': 1153}\n",
      "{'book': '1CO', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 624, 'Ilokano_rows': 710, 'aligned_pairs': 710}\n",
      "{'book': '1JN', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 161, 'Ilokano_rows': 167, 'aligned_pairs': 167}\n",
      "{'book': '1KI', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 1163, 'Ilokano_rows': 1369, 'aligned_pairs': 1369}\n",
      "{'book': '1MA', 'Pangasinense_files': 1, 'Ilokano_files': 0, 'Pangasinense_rows': 1187, 'Ilokano_rows': 0, 'aligned_pairs': 1187}\n",
      "{'book': '1PE', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 159, 'Ilokano_rows': 192, 'aligned_pairs': 192}\n",
      "[1] [1CH] Pangasinense(PNPV_1CH_raw#1): Si Adan so ama nen Set ya ama nen Enos, tan si Enos so ama nen Kenan.  ||  Ilokano(RIPV_1CH_raw#1): Ni Adan ti ama ni Set nga ama ni Enos.\n",
      "[2] [1CH] Pangasinense(PNPV_1CH_raw#2): Si Kenan so ama nen Mahalalel ya ama nen Jared, tan si Jared so ama nen Enoc ya ama nen Matusalem.  ||  Ilokano(RIPV_1CH_raw#2): Ni Enos ti ama ni Kenan.\n",
      "[3] [1CH] Pangasinense(PNPV_1CH_raw#3): Si Matusalem so ama nen Lamec ya ama nen Noe.  ||  Ilokano(RIPV_1CH_raw#3): Ni Kenan ti ama ni Mahalalel nga ama ni Jared.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def _natural_key(s: str):\n",
    "    # natural sort so 10 comes after 9\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
    "\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        nc = c.replace(\"\\ufeff\", \"\").strip().lower()  # strip BOM + normalize\n",
    "        new_cols[c] = nc\n",
    "    return df.rename(columns=new_cols)\n",
    "\n",
    "def _is_missing_str(s) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    return t == \"\" or t.lower() in {\"nan\", \"none\", \"null\"}\n",
    "\n",
    "def read_csv_sentences(path, keep_empty=False):\n",
    "    \"\"\"\n",
    "    Read a CSV that should contain a single column 'sentence' (case/BOM tolerant).\n",
    "    Returns list[str]. If keep_empty=True, preserves blank lines as \"\".\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    df = None\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=str, encoding=enc, on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to read CSV: {path}\\n{last_err}\")\n",
    "\n",
    "    df = _normalize_columns(df)\n",
    "\n",
    "    if \"sentence\" not in df.columns:\n",
    "        if df.shape[1] == 1:\n",
    "            only = df.columns[0]\n",
    "            df = df.rename(columns={only: \"sentence\"})\n",
    "        else:\n",
    "            raise ValueError(f\"'sentence' column not found in {path}. Columns: {list(df.columns)}\")\n",
    "\n",
    "    s = (\n",
    "        df[\"sentence\"]\n",
    "        .astype(str)\n",
    "        .map(lambda x: x.replace(\"\\r\", \" \").strip())\n",
    "        .tolist()\n",
    "    )\n",
    "    if not keep_empty:\n",
    "        s = [v for v in s if not _is_missing_str(v)]\n",
    "    else:\n",
    "        s = [v if not _is_missing_str(v) else \"\" for v in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_csvs(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively list CSVs under root_dir, natural-sorted for determinism.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(root_dir, \"**\", \"*.csv\")\n",
    "    files = [p for p in glob.glob(pattern, recursive=True) if os.path.isfile(p)]\n",
    "    files.sort(key=_natural_key)\n",
    "    return files\n",
    "\n",
    "def language_name_from_dir(dir_path):\n",
    "    return os.path.basename(os.path.normpath(dir_path)) or \"Language\"\n",
    "\n",
    "def ensure_ext(base, ext):\n",
    "    b, _ = os.path.splitext(base)\n",
    "    return b + ext\n",
    "\n",
    "\n",
    "def write_outputs(df, out_base):\n",
    "    out_csv = ensure_ext(out_base, \".csv\")\n",
    "    out_txt = ensure_ext(out_base, \".txt\")\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, r in df.iterrows():\n",
    "            book = str(r[cols[0]])\n",
    "            l1_id, l1_sent, l2_id, l2_sent = r[cols[1]], r[cols[2]], r[cols[3]], r[cols[4]]\n",
    "\n",
    "            f.write(f\"[{book}] {cols[2]} ({l1_id})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(l1_sent) else str(l1_sent)}\\n\")\n",
    "            f.write(f\"{cols[4]} ({l2_id})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(l2_sent) else str(l2_sent)}\\n\\n\")\n",
    "\n",
    "    return out_csv, out_txt\n",
    "\n",
    "\n",
    "BOOK_RE = re.compile(r'(?<![A-Z0-9])([1-3]?[A-Z]{2,3})(?![A-Z])')\n",
    "\n",
    "def extract_book_code(path):\n",
    "    \"\"\"\n",
    "    Extract the book code strictly as the token between the first and second underscore.\n",
    "    Examples:\n",
    "      'MBB05_MAT_raw_lines.csv'   -> 'MAT'\n",
    "      'VTSP_1CO_raw_lines.csv'    -> '1CO'\n",
    "    Returns None if the filename doesn't have at least two underscores.\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(os.path.basename(path))[0]\n",
    "    parts = base.split('_')\n",
    "    if len(parts) >= 3:             \n",
    "        return parts[1].upper()      \n",
    "    return None\n",
    "\n",
    "def build_book_map(root_dir):\n",
    "    \"\"\"\n",
    "    Returns dict: {book_code: [csv_paths_for_that_book_sorted]}\n",
    "    \"\"\"\n",
    "    book_map = {}\n",
    "    for f in list_csvs(root_dir):\n",
    "        code = extract_book_code(f)\n",
    "        if code:\n",
    "            book_map.setdefault(code, []).append(f)\n",
    "    for k in book_map:\n",
    "        book_map[k].sort(key=_natural_key)\n",
    "    return book_map\n",
    "\n",
    "def read_book_entries(file_list, keep_empty=False):\n",
    "    \"\"\"\n",
    "    For a given book, read all files (in order) and return a list of entries:\n",
    "    [{'id': '<file_stem>#<1based>', 'sentence': '<text>'}, ...]\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    for f in file_list:\n",
    "        stem = os.path.splitext(os.path.basename(f))[0]\n",
    "        sents = read_csv_sentences(f, keep_empty=keep_empty)\n",
    "        for i, sent in enumerate(sents, start=1):\n",
    "            entries.append({\"id\": f\"{stem}#{i}\", \"sentence\": sent})\n",
    "    return entries\n",
    "\n",
    "def merge_pair_by_book(lang1_dir, lang2_dir, out_base_name,\n",
    "                       out_dir=None, use_fixed_headers=False, keep_empty=False):\n",
    "    lang1_dir = os.path.normpath(lang1_dir)\n",
    "    lang2_dir = os.path.normpath(lang2_dir)\n",
    "\n",
    "    l1_name = \"Language 1\" if use_fixed_headers else language_name_from_dir(lang1_dir)\n",
    "    l2_name = \"Language 2\" if use_fixed_headers else language_name_from_dir(lang2_dir)\n",
    "\n",
    "    l1_books = build_book_map(lang1_dir)\n",
    "    l2_books = build_book_map(lang2_dir)\n",
    "\n",
    "    all_books = sorted(set(l1_books.keys()) | set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    rows = []\n",
    "    per_book_stats = []\n",
    "\n",
    "    for bk in all_books:\n",
    "        l1_files = l1_books.get(bk, [])\n",
    "        l2_files = l2_books.get(bk, [])\n",
    "\n",
    "        s1 = read_book_entries(l1_files, keep_empty=keep_empty)\n",
    "        s2 = read_book_entries(l2_files, keep_empty=keep_empty)\n",
    "\n",
    "        filler1 = {\"id\": \"N/A\", \"sentence\": \"N/A\"}\n",
    "        filler2 = {\"id\": \"N/A\", \"sentence\": \"N/A\"}\n",
    "\n",
    "        for e1, e2 in zip_longest(s1, s2, fillvalue=None):\n",
    "            e1 = e1 if e1 is not None else filler1\n",
    "            e2 = e2 if e2 is not None else filler2\n",
    "            rows.append({\n",
    "                \"book\": bk,\n",
    "                f\"{l1_name} ID\": e1[\"id\"],\n",
    "                f\"{l1_name}\": e1[\"sentence\"],\n",
    "                f\"{l2_name} ID\": e2[\"id\"],\n",
    "                f\"{l2_name}\": e2[\"sentence\"],\n",
    "            })\n",
    "\n",
    "        per_book_stats.append({\n",
    "            \"book\": bk,\n",
    "            f\"{l1_name}_files\": len(l1_files),\n",
    "            f\"{l2_name}_files\": len(l2_files),\n",
    "            f\"{l1_name}_rows\": len(s1),\n",
    "            f\"{l2_name}_rows\": len(s2),\n",
    "            \"aligned_pairs\": max(len(s1), len(s2)),  \n",
    "        })\n",
    "\n",
    "    cols = [\"book\", f\"{l1_name} ID\", f\"{l1_name}\", f\"{l2_name} ID\", f\"{l2_name}\"]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    out_base = out_base_name if out_dir is None else os.path.join(out_dir, out_base_name)\n",
    "    out_csv, out_txt = write_outputs(df, out_base)\n",
    "\n",
    "    common_books = sorted(set(l1_books.keys()) & set(l2_books.keys()), key=_natural_key)\n",
    "    missing_l1 = sorted(set(l2_books.keys()) - set(l1_books.keys()), key=_natural_key)\n",
    "    missing_l2 = sorted(set(l1_books.keys()) - set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    print(f\"Books (total): {len(all_books)}; Common books: {len(common_books)}\")\n",
    "    if missing_l1:\n",
    "        print(f\"Books only in {l2_name}: {missing_l1[:10]}{'...' if len(missing_l1)>10 else ''}\")\n",
    "    if missing_l2:\n",
    "        print(f\"Books only in {l1_name}: {missing_l2[:10]}{'...' if len(missing_l2)>10 else ''}\")\n",
    "    print(f\"Total rows (after padding with N/A): {len(df)}\")\n",
    "    print(f\"CSV → {out_csv}\")\n",
    "    print(f\"TXT → {out_txt}\")\n",
    "\n",
    "    print(\"\\nPer-book stats (first 6):\")\n",
    "    for s in per_book_stats[:6]:\n",
    "        print(s)\n",
    "\n",
    "    for i in range(min(3, len(df))):\n",
    "        r = df.iloc[i]\n",
    "        print(f\"[{i+1}] [{r['book']}] {l1_name}({r[f'{l1_name} ID']}): {r[f'{l1_name}']}  ||  \"\n",
    "              f\"{l2_name}({r[f'{l2_name} ID']}): {r[f'{l2_name}']}\")\n",
    "\n",
    "    return out_csv, out_txt, df\n",
    "\n",
    "results = []\n",
    "for pair in PAIRS:\n",
    "    csv_path, txt_path, df_out = merge_pair_by_book(\n",
    "        pair[\"lang1_dir\"], pair[\"lang2_dir\"],\n",
    "        out_base_name=pair[\"out_txt\"],\n",
    "        out_dir=OUT_DIR,\n",
    "        use_fixed_headers=USE_FIXED_HEADERS,\n",
    "        keep_empty=KEEP_EMPTY_LINES,\n",
    "    )\n",
    "    results.append((csv_path, txt_path, len(df_out)))\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
