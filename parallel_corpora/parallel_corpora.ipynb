{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697467f1",
   "metadata": {},
   "source": [
    "# Parallel Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecea2a0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3245aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Iterable, Union\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d8fe4",
   "metadata": {},
   "source": [
    "## By verse\n",
    "This script provides a reusable pipeline for building parallel corpora of Bible translations aligned by verse, including cases where verse identifiers are merged across languages. For each configured language pair, it loads all verse-aligned files from two input directories, parses them into dictionaries keyed by USFM identifiers (e.g., 1CO.5.12 or merged keys like 1CO.5.12+1CO.5.13), and automatically harmonizes verse boundaries between languages. When one language merges consecutive verses while the other keeps them separate, the script detects the overlap and merges the corresponding verses to maintain alignment. It then constructs a unified, verse-by-verse (or merged-verse) mapping, fills in missing entries with a placeholder token, and exports two synchronized outputs: a plain-text file with interleaved verse pairs and a CSV file containing structured metadata (book, chapter, verse label, and both language texts). The resulting corpus ensures consistent alignment even when translations differ in verse segmentation, making it suitable for multilingual text mining, translation studies, or machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/sentence/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_verse.txt\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_verse.txt\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/cj/parsed/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_verse.txt\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_verse.txt\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ddaa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Kapampangan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Bikolano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Tausug True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Yami True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Ilokano True\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e16520",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 260\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang1_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m lang2_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach pair must include \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang1_dir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang2_dir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pair\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang1_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang1_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang2_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang2_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_txt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     all_summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone processing all pairs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 208\u001b[0m, in \u001b[0;36mprocess_pair\u001b[0;34m(lang1_dir, lang2_dir, out_txt, missing)\u001b[0m\n\u001b[1;32m    206\u001b[0m key, _, _, _ \u001b[38;5;241m=\u001b[39m group_label(atoms)\n\u001b[1;32m    207\u001b[0m t1 \u001b[38;5;241m=\u001b[39m assemble_text_for_group(lang1, atoms, missing)\n\u001b[0;32m--> 208\u001b[0m t2 \u001b[38;5;241m=\u001b[39m \u001b[43massemble_text_for_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m lines_out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m lines_out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 145\u001b[0m, in \u001b[0;36massemble_text_for_group\u001b[0;34m(lang_map, group_atoms, missing)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massemble_text_for_group\u001b[39m(lang_map: Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    139\u001b[0m                             group_atoms: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    140\u001b[0m                             missing: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    If lang has a matching merged key (any order), use it. Otherwise join member-verse texts.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    When joining, preserve verse order; insert `missing` for absent members.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     sig_to_key \u001b[38;5;241m=\u001b[39m \u001b[43mmap_atoms_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(group_atoms)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m sig_to_key:\n",
      "Cell \u001b[0;32mIn[21], line 133\u001b[0m, in \u001b[0;36mmap_atoms_signature\u001b[0;34m(keys_map)\u001b[0m\n\u001b[1;32m    131\u001b[0m sig \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys_map:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_merged_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    134\u001b[0m         atoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(split_merged(k))\n\u001b[1;32m    135\u001b[0m         sig[atoms] \u001b[38;5;241m=\u001b[39m k\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# helpers\n",
    "\n",
    "DEFAULT_MISSING = '\"N/A\"'\n",
    "MERGE_JOIN_SEP   = \" \"         \n",
    "KEY_JOIN_SEP     = \"+\"        \n",
    "\n",
    "LINE_RE = re.compile(\n",
    "    r'^\\s*(([0-9A-Z]+\\.\\d+\\.\\d+)(?:\\+[0-9A-Z]+\\.\\d+\\.\\d+)*)\\s+(.*\\S)\\s*$'\n",
    ")\n",
    "\n",
    "def _to_path(p):\n",
    "    return None if p is None else (p if isinstance(p, Path) else Path(p))\n",
    "\n",
    "def is_merged_key(usfm: str) -> bool:\n",
    "    return KEY_JOIN_SEP in usfm\n",
    "\n",
    "def split_merged(usfm: str) -> list[str]:\n",
    "    return usfm.split(KEY_JOIN_SEP)\n",
    "\n",
    "def usfm_sort_key(usfm: str):\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if parts else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return (book, chap, verse)\n",
    "\n",
    "def normalize_group_key(atom_ids: list[str]) -> str:\n",
    "    atoms_sorted = sorted(atom_ids, key=usfm_sort_key)\n",
    "    return KEY_JOIN_SEP.join(atoms_sorted)\n",
    "\n",
    "def split_usfm(usfm: str):\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if len(parts) > 0 else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return book, chap, verse\n",
    "\n",
    "def _last_dir_name(p: Path) -> str:\n",
    "    return p.name or p.parent.name\n",
    "\n",
    "\n",
    "def parse_txt_file(fp: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse a single .txt file into {usfm_or_merged_key: text}.\n",
    "    Allows merged identifiers joined by '+'.\n",
    "    \"\"\"\n",
    "    d: Dict[str, str] = {}\n",
    "    with fp.open(\"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "        for raw in f:\n",
    "            m = LINE_RE.match(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            key, text = m.group(1), m.group(3).strip()\n",
    "            d[key] = text\n",
    "    return d\n",
    "\n",
    "def load_folder(folder: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Merge all *.txt files in a folder into a single map {key: text}.\n",
    "    Later files overwrite earlier ones on the same key (simple last-wins).\n",
    "    \"\"\"\n",
    "    combined: Dict[str, str] = {}\n",
    "    txts = sorted(folder.glob(\"*.txt\"))\n",
    "    for fp in txts:\n",
    "        part = parse_txt_file(fp)\n",
    "        combined.update(part)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def collect_groups(lang_map: Dict[str, str]) -> list[set[str]]:\n",
    "    \"\"\"Return list of sets of atomic IDs for all merged keys in a language map.\"\"\"\n",
    "    groups = []\n",
    "    for k in lang_map:\n",
    "        if is_merged_key(k):\n",
    "            groups.append(set(split_merged(k)))\n",
    "    return groups\n",
    "\n",
    "def union_overlapping_groups(group_sets: list[set[str]]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Given a list of sets of atomic IDs, union any that overlap.\n",
    "    Returns list of sorted lists of atomic IDs (stable order by USFM).\n",
    "    \"\"\"\n",
    "    groups = [set(g) for g in group_sets]\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        out = []\n",
    "        while groups:\n",
    "            g = groups.pop()\n",
    "            merged = True\n",
    "            while merged:\n",
    "                merged = False\n",
    "                for i in range(len(groups)-1, -1, -1):\n",
    "                    if g & groups[i]:\n",
    "                        g |= groups[i]\n",
    "                        groups.pop(i)\n",
    "                        merged = True\n",
    "                        changed = True\n",
    "            out.append(g)\n",
    "        groups = out\n",
    "    return [sorted(list(g), key=usfm_sort_key) for g in groups]\n",
    "\n",
    "def all_final_groups(lang1: Dict[str,str], lang2: Dict[str,str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Build the final grouping units (lists of atomic IDs) by taking:\n",
    "      - all merged groupings from both langs (unioning overlaps),\n",
    "      - plus singleton groups for any remaining atom not covered.\n",
    "    \"\"\"\n",
    "    g1 = collect_groups(lang1)\n",
    "    g2 = collect_groups(lang2)\n",
    "    merged_groups = union_overlapping_groups(g1 + g2) \n",
    "\n",
    "    covered = set(a for grp in merged_groups for a in grp)\n",
    "\n",
    "    all_atoms = set()\n",
    "    for k in list(lang1.keys()) + list(lang2.keys()):\n",
    "        if is_merged_key(k):\n",
    "            all_atoms.update(split_merged(k))\n",
    "        else:\n",
    "            all_atoms.add(k)\n",
    "\n",
    "    singletons = [[a] for a in sorted(all_atoms - covered, key=usfm_sort_key)]\n",
    "    return merged_groups + singletons\n",
    "\n",
    "def map_atoms_signature(keys_map: Dict[str,str]) -> Dict[frozenset, str]:\n",
    "    \"\"\"\n",
    "    For quick lookup: if a language already has a merged key whose atoms set equals a group,\n",
    "    we want to reuse that exact key string (and its text).\n",
    "    Returns {frozenset(atom_ids): merged_key_string}\n",
    "    \"\"\"\n",
    "    sig = {}\n",
    "    for k in keys_map:\n",
    "        if is_merged_key(k):\n",
    "            atoms = frozenset(split_merged(k))\n",
    "            sig[atoms] = k\n",
    "    return sig\n",
    "\n",
    "def assemble_text_for_group(lang_map: Dict[str,str],\n",
    "                            group_atoms: list[str],\n",
    "                            missing: str) -> str:\n",
    "    \"\"\"\n",
    "    If lang has a matching merged key (any order), use it. Otherwise join member-verse texts.\n",
    "    When joining, preserve verse order; insert `missing` for absent members.\n",
    "    \"\"\"\n",
    "    sig_to_key = map_atoms_signature(lang_map)\n",
    "    sig = frozenset(group_atoms)\n",
    "    if sig in sig_to_key:\n",
    "        return lang_map[sig_to_key[sig]]\n",
    "\n",
    "    parts = []\n",
    "    for a in group_atoms:\n",
    "        parts.append(lang_map.get(a, missing))\n",
    "    return MERGE_JOIN_SEP.join(parts)\n",
    "\n",
    "def group_label(group_atoms: list[str]) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Build output columns for (usfm_key, book, chapter, verse_label) from a group.\n",
    "    - usfm_key: normalized merged key 'a+b+...'\n",
    "    - book, chapter: taken from the first atomic ID (if consistent)\n",
    "    - verse_label: '12+13' if same chapter; otherwise 'mixed'\n",
    "    \"\"\"\n",
    "    atoms_sorted = sorted(group_atoms, key=usfm_sort_key)\n",
    "    key = normalize_group_key(atoms_sorted)\n",
    "\n",
    "    b0, c0, v0 = split_usfm(atoms_sorted[0])\n",
    "    same_book_chapter = all(split_usfm(a)[0] == b0 and split_usfm(a)[1] == c0 for a in atoms_sorted)\n",
    "    if same_book_chapter:\n",
    "        verse_label = KEY_JOIN_SEP.join(str(split_usfm(a)[2]) for a in atoms_sorted)\n",
    "        book, chap = b0, c0\n",
    "    else:\n",
    "        verse_label = \"mixed\"\n",
    "        book, chap = b0, c0  \n",
    "\n",
    "    return key, book, str(chap), verse_label\n",
    "\n",
    "\n",
    "def process_pair(\n",
    "    lang1_dir: Path,\n",
    "    lang2_dir: Path,\n",
    "    out_txt: Path,\n",
    "    missing: str = DEFAULT_MISSING,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process one language pair and write:\n",
    "      - TXT with stacked lines per GROUP-KEY:  \"<KEY> <lang1_text>\" then \"<KEY> <lang2_text>\"\n",
    "      - CSV with columns: usfm_key, book, verse(s), chapter, language 1, language2\n",
    "    \"\"\"\n",
    "    assert lang1_dir.is_dir(), f\"Not a directory: {lang1_dir}\"\n",
    "    assert lang2_dir.is_dir(), f\"Not a directory: {lang2_dir}\"\n",
    "\n",
    "    if out_txt is None:\n",
    "        name1 = _last_dir_name(lang1_dir)\n",
    "        name2 = _last_dir_name(lang2_dir)\n",
    "        out_txt = Path(f\"{name1}_{name2}_verse.txt\")\n",
    "    out_csv = out_txt.with_suffix(\".csv\")\n",
    "\n",
    "    lang1 = load_folder(lang1_dir)\n",
    "    lang2 = load_folder(lang2_dir)\n",
    "\n",
    "    groups = all_final_groups(lang1, lang2)\n",
    "\n",
    "    groups.sort(key=lambda g: usfm_sort_key(sorted(g, key=usfm_sort_key)[0]))\n",
    "\n",
    "    lines_out = []\n",
    "    for atoms in groups:\n",
    "        key, _, _, _ = group_label(atoms)\n",
    "        t1 = assemble_text_for_group(lang1, atoms, missing)\n",
    "        t2 = assemble_text_for_group(lang2, atoms, missing)\n",
    "        lines_out.append(f\"{key} {t1}\")\n",
    "        lines_out.append(f\"{key} {t2}\")\n",
    "    out_txt.write_text(\"\\n\".join(lines_out) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "    with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"usfm\", \"book\", \"verse\", \"chapter\", \"language 1\", \"language2\"])\n",
    "        for atoms in groups:\n",
    "            key, book, chap, verse_label = group_label(atoms)\n",
    "            t1 = assemble_text_for_group(lang1, atoms, missing)\n",
    "            t2 = assemble_text_for_group(lang2, atoms, missing)\n",
    "            w.writerow([key, book, verse_label, chap, t1, t2])\n",
    "\n",
    "    all_atoms_lang1 = set()\n",
    "    for k in lang1:\n",
    "        all_atoms_lang1.update(split_merged(k) if is_merged_key(k) else [k])\n",
    "    all_atoms_lang2 = set()\n",
    "    for k in lang2:\n",
    "        all_atoms_lang2.update(split_merged(k) if is_merged_key(k) else [k])\n",
    "\n",
    "    group_keys = [normalize_group_key(g) for g in groups]\n",
    "    summary = {\n",
    "        \"out_txt\": str(out_txt),\n",
    "        \"out_csv\": str(out_csv),\n",
    "        \"txt_lines_written\": len(lines_out),\n",
    "        \"csv_rows_written\": len(groups) + 1,  # +1 for header\n",
    "        \"groups_total\": len(groups),\n",
    "        \"missing_in_lang1_atoms\": sum(1 for g in groups for a in g if a not in all_atoms_lang1),\n",
    "        \"missing_in_lang2_atoms\": sum(1 for g in groups for a in g if a not in all_atoms_lang2),\n",
    "        \"missing_token\": missing,\n",
    "        \"merge_join_sep\": MERGE_JOIN_SEP,\n",
    "        \"key_join_sep\": KEY_JOIN_SEP,\n",
    "    }\n",
    "    print(\n",
    "        f\"Processed {out_txt.name} & {out_csv.name} | groups: {summary['groups_total']} | \"\n",
    "        f\"missing_atoms(lang1): {summary['missing_in_lang1_atoms']} | missing_atoms(lang2): {summary['missing_in_lang2_atoms']}\"\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "# main\n",
    "all_summaries = []\n",
    "for cfg in PAIRS:\n",
    "    lang1_dir = _to_path(cfg.get(\"lang1_dir\"))\n",
    "    lang2_dir = _to_path(cfg.get(\"lang2_dir\"))\n",
    "    out_txt   = _to_path(cfg.get(\"out_txt\"))\n",
    "    missing   = cfg.get(\"missing\", DEFAULT_MISSING)\n",
    "\n",
    "    if lang1_dir is None or lang2_dir is None:\n",
    "        raise ValueError(\"Each pair must include 'lang1_dir' and 'lang2_dir'.\")\n",
    "\n",
    "    summary = process_pair(\n",
    "        lang1_dir=lang1_dir,\n",
    "        lang2_dir=lang2_dir,\n",
    "        out_txt=out_txt,\n",
    "        missing=missing,\n",
    "    )\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nDone processing all pairs.\")\n",
    "for s in all_summaries:\n",
    "    print(f\"- Wrote TXT: {s['out_txt']} | CSV: {s['out_csv']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e76df",
   "metadata": {},
   "source": [
    "## By sentence\n",
    "\n",
    "This code builds parallel corpora for multiple language pairs by aligning sentence-level CSV files book by book. The PAIRS list defines which languages to pair, with input directories and output filenames. Helper functions handle reading CSVs, normalizing columns, detecting missing values, and natural sorting of files.\n",
    "\n",
    "The script matches books between two languages by first building a map of all available books in each directory, using the book code extracted from filenames (e.g., MAT, 1CO). It then takes the union of both sets of book codes so every book that appears in either language is included. If a book exists in one language but not the other, the missing side is padded with placeholder entries (N/A). This ensures that all books are represented, with sentences aligned where both languages exist and gaps clearly marked when they do not.\n",
    "\n",
    "Each book’s sentences are extracted, aligned with padding (N/A if missing), and combined into a DataFrame. The results are written both as CSV (for structured analysis) and TXT (for human-readable format). Along the way, the script logs statistics on books, rows, and missing entries. In short, it automates the process of matching bilingual sentence data into clean, aligned outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd694bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_sentence\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_sentence\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/by_sentence/yna/sentence/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/yna/sentence/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/cj/sentence/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_sentence\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/by_sentence/cj/sentence/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/by_sentence/trish/sentence/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_sentence\",\n",
    "    },\n",
    "]\n",
    "\n",
    "OUT_DIR = None\n",
    "USE_FIXED_HEADERS = False\n",
    "KEEP_EMPTY_LINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b80bccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Kapampangan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Bikolano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Tausug True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/yna/sentence/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Yami True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/cj/sentence/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/by_sentence/trish/sentence/Ilokano True\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9e1bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books (total): 77; Common books: 77\n",
      "Total rows (after padding with N/A): 53250\n",
      "CSV → tagalog_kapampangan_sentence.csv\n",
      "TXT → tagalog_kapampangan_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1133, 'Kapampangan_rows': 1262, 'aligned_pairs': 1262}\n",
      "{'book': '1CO', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 606, 'Kapampangan_rows': 609, 'aligned_pairs': 609}\n",
      "{'book': '1JN', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 156, 'Kapampangan_rows': 159, 'aligned_pairs': 159}\n",
      "{'book': '1KI', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1361, 'Kapampangan_rows': 1263, 'aligned_pairs': 1361}\n",
      "{'book': '1MA', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 1322, 'Kapampangan_rows': 1208, 'aligned_pairs': 1322}\n",
      "{'book': '1PE', 'Tagalog_files': 1, 'Kapampangan_files': 1, 'Tagalog_rows': 154, 'Kapampangan_rows': 156, 'aligned_pairs': 156}\n",
      "[1] [1CH] Tagalog(MBB05_1CH_raw#1): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Kapampangan(PmPV_1CH_raw#1): Anak neng Adan i Set, at anak neng Set i Enos.\n",
      "[2] [1CH] Tagalog(MBB05_1CH_raw#2): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Kapampangan(PmPV_1CH_raw#2): Anak neng Enos i Kenan, at anak neng Kenan i Mahalalel.\n",
      "[3] [1CH] Tagalog(MBB05_1CH_raw#3): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Kapampangan(PmPV_1CH_raw#3): Anak neng Mahalalel i Jared.\n",
      "Books (total): 77; Common books: 77\n",
      "Total rows (after padding with N/A): 51912\n",
      "CSV → tagalog_bikolano_sentence.csv\n",
      "TXT → tagalog_bikolano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1133, 'Bikolano_rows': 984, 'aligned_pairs': 1133}\n",
      "{'book': '1CO', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 606, 'Bikolano_rows': 608, 'aligned_pairs': 608}\n",
      "{'book': '1JN', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 156, 'Bikolano_rows': 143, 'aligned_pairs': 156}\n",
      "{'book': '1KI', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1361, 'Bikolano_rows': 1180, 'aligned_pairs': 1361}\n",
      "{'book': '1MA', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 1322, 'Bikolano_rows': 1168, 'aligned_pairs': 1322}\n",
      "{'book': '1PE', 'Tagalog_files': 1, 'Bikolano_files': 1, 'Tagalog_rows': 154, 'Bikolano_rows': 139, 'aligned_pairs': 154}\n",
      "[1] [1CH] Tagalog(MBB05_1CH_raw#1): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Bikolano(MBBBIK92_1CH_raw#1): Si Adan iyo an ama ni Set, asin si Set iyo an ama ni Enos na ama ni Kenan.\n",
      "[2] [1CH] Tagalog(MBB05_1CH_raw#2): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Bikolano(MBBBIK92_1CH_raw#2): Si Kenan iyo an ama ni Mahalalel na ama ni Jared.\n",
      "[3] [1CH] Tagalog(MBB05_1CH_raw#3): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Bikolano(MBBBIK92_1CH_raw#3): Si Jared iyo an ama ni Enoc na ama ni Metusela; si Metusela iyo an ama ni Lamec na iyo an ama ni Noe.\n",
      "Books (total): 66; Common books: 66\n",
      "Total rows (after padding with N/A): 35818\n",
      "CSV → cebuano_spanish_sentence.csv\n",
      "TXT → cebuano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 884, 'Spanish_rows': 850, 'aligned_pairs': 884}\n",
      "{'book': '1CO', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 522, 'Spanish_rows': 446, 'aligned_pairs': 522}\n",
      "{'book': '1JN', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 123, 'Spanish_rows': 123, 'aligned_pairs': 123}\n",
      "{'book': '1KI', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 963, 'Spanish_rows': 980, 'aligned_pairs': 980}\n",
      "{'book': '1PE', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 103, 'Spanish_rows': 87, 'aligned_pairs': 103}\n",
      "{'book': '1SA', 'Cebuano_files': 1, 'Spanish_files': 1, 'Cebuano_rows': 1188, 'Spanish_rows': 1144, 'aligned_pairs': 1188}\n",
      "[1] [1CH] Cebuano(ABCEB_1CH_raw#1): Si Adan, si Set, si Enos, si Kenan, si Mahalalel, si Jared, si Enoc, si Metusela, si Lamec, si Noe, si Sem, si Ham ug si Jafet.  ||  Spanish(RVR95_1CH_raw#1): Adán, Set, Enós, Cainán, Mahalaleel, Jared, Enoc, Matusalén, Lamec, Noé, Sem, Cam y Jafet.\n",
      "[2] [1CH] Cebuano(ABCEB_1CH_raw#2): Ang mga anak nga lalaki ni Jafet: si Gomer, si Magog, si Madai, si Javan, si Tubal, si Mesec ug si Tiras.  ||  Spanish(RVR95_1CH_raw#2): Los hijos de Jafet: Gomer, Magog, Madai, Javán, Tubal, Mesec y Tiras.\n",
      "[3] [1CH] Cebuano(ABCEB_1CH_raw#3): Ang mga anak nga lalaki ni Gomer: si Askenas, si Rifat ug si Togarma.  ||  Spanish(RVR95_1CH_raw#3): Los hijos de Gomer: Askenaz, Rifat y Togarma.\n",
      "Books (total): 66; Common books: 27\n",
      "Books only in Cebuano: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total rows (after padding with N/A): 42469\n",
      "CSV → cebuano_tausug_sentence.csv\n",
      "TXT → cebuano_tausug_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 884, 'Tausug_rows': 0, 'aligned_pairs': 884}\n",
      "{'book': '1CO', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 522, 'Tausug_rows': 901, 'aligned_pairs': 901}\n",
      "{'book': '1JN', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 123, 'Tausug_rows': 187, 'aligned_pairs': 187}\n",
      "{'book': '1KI', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 963, 'Tausug_rows': 0, 'aligned_pairs': 963}\n",
      "{'book': '1PE', 'Cebuano_files': 1, 'Tausug_files': 1, 'Cebuano_rows': 103, 'Tausug_rows': 235, 'aligned_pairs': 235}\n",
      "{'book': '1SA', 'Cebuano_files': 1, 'Tausug_files': 0, 'Cebuano_rows': 1188, 'Tausug_rows': 0, 'aligned_pairs': 1188}\n",
      "[1] [1CH] Cebuano(ABCEB_1CH_raw#1): Si Adan, si Set, si Enos, si Kenan, si Mahalalel, si Jared, si Enoc, si Metusela, si Lamec, si Noe, si Sem, si Ham ug si Jafet.  ||  Tausug(N/A): N/A\n",
      "[2] [1CH] Cebuano(ABCEB_1CH_raw#2): Ang mga anak nga lalaki ni Jafet: si Gomer, si Magog, si Madai, si Javan, si Tubal, si Mesec ug si Tiras.  ||  Tausug(N/A): N/A\n",
      "[3] [1CH] Cebuano(ABCEB_1CH_raw#3): Ang mga anak nga lalaki ni Gomer: si Askenas, si Rifat ug si Togarma.  ||  Tausug(N/A): N/A\n",
      "Books (total): 66; Common books: 27\n",
      "Books only in Spanish: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total rows (after padding with N/A): 38004\n",
      "CSV → chavacano_spanish_sentence.csv\n",
      "TXT → chavacano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 850, 'aligned_pairs': 850}\n",
      "{'book': '1CO', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 749, 'Spanish_rows': 446, 'aligned_pairs': 749}\n",
      "{'book': '1JN', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 154, 'Spanish_rows': 123, 'aligned_pairs': 154}\n",
      "{'book': '1KI', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 980, 'aligned_pairs': 980}\n",
      "{'book': '1PE', 'Chavacano_files': 1, 'Spanish_files': 1, 'Chavacano_rows': 187, 'Spanish_rows': 87, 'aligned_pairs': 187}\n",
      "{'book': '1SA', 'Chavacano_files': 0, 'Spanish_files': 1, 'Chavacano_rows': 0, 'Spanish_rows': 1144, 'aligned_pairs': 1144}\n",
      "[1] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#1): Adán, Set, Enós, Cainán, Mahalaleel, Jared, Enoc, Matusalén, Lamec, Noé, Sem, Cam y Jafet.\n",
      "[2] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#2): Los hijos de Jafet: Gomer, Magog, Madai, Javán, Tubal, Mesec y Tiras.\n",
      "[3] [1CH] Chavacano(N/A): N/A  ||  Spanish(RVR95_1CH_raw#3): Los hijos de Gomer: Askenaz, Rifat y Togarma.\n",
      "Books (total): 29; Common books: 27\n",
      "Books only in Ivatan: ['PRO', 'PSA']\n",
      "Total rows (after padding with N/A): 18182\n",
      "CSV → ivatan_yami_sentence.csv\n",
      "TXT → ivatan_yami_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CO', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 500, 'Yami_rows': 944, 'aligned_pairs': 944}\n",
      "{'book': '1JN', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 112, 'Yami_rows': 236, 'aligned_pairs': 236}\n",
      "{'book': '1PE', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 111, 'Yami_rows': 259, 'aligned_pairs': 259}\n",
      "{'book': '1TH', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 63, 'Yami_rows': 173, 'aligned_pairs': 173}\n",
      "{'book': '1TI', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 126, 'Yami_rows': 247, 'aligned_pairs': 247}\n",
      "{'book': '2CO', 'Ivatan_files': 1, 'Yami_files': 1, 'Ivatan_rows': 273, 'Yami_rows': 612, 'aligned_pairs': 612}\n",
      "[1] [1CO] Ivatan(VTSP_1CO_raw#1): Si Pablo ako a tinawagan no Dios a asa ka apostol ni Jesu Cristo a makayamot do inolay na as kani Sostenes a kakteh ta.  ||  Yami(SNT_1CO_raw#1): Yaken rana am si Pawlo ko a yana nipamnekan ni Yeso Kizisto a amlivolivon no ciriciring na a lamlamsoy na ni Ama ta do to.\n",
      "[2] [1CO] Ivatan(VTSP_1CO_raw#2): Tayto kami a maytolas dinio do timban aya no Dios do Corinto, dinio a pinayvadiw na a manamonamo a makayamot di Jesu Cristo as tinawagan na a tawotawo na a akma sira so kadwan do matatarek a logar a tayto a omanianib so Apo taya a si Jesu Cristo a iya so Apohen ta atavo.  ||  Yami(SNT_1CO_raw#2): Yaken rana kano akma tey kakteh do cinai si Sotenay am namen ipeyteygami inyo a sinjya do Kedinto a kyokay.\n",
      "[3] [1CO] Ivatan(VTSP_1CO_raw#3): Bendisionan kamo pa no Dios Ama as kano Apo taya a si Jesu Cristo as kapakarawat nio pa so grasia kano kaydamnayan no aktokto.  ||  Yami(SNT_1CO_raw#3): Inyo rana kano yamakakaday do peycyeylilyan na a macikaop ji Kizisto a tao am yatamo meyyangangay a na nipamnekan ni Ama ta do to a italamozong na.\n",
      "Books (total): 77; Common books: 66\n",
      "Books only in Pangasinense: ['1MA', '2MA', 'BAR', 'BEL', 'JDT', 'LJE', 'S3Y', 'SIR', 'SUS', 'TOB']...\n",
      "Total rows (after padding with N/A): 55889\n",
      "CSV → pangasinense_ilokano_sentence.csv\n",
      "TXT → pangasinense_ilokano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 1080, 'Ilokano_rows': 1156, 'aligned_pairs': 1156}\n",
      "{'book': '1CO', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 624, 'Ilokano_rows': 710, 'aligned_pairs': 710}\n",
      "{'book': '1JN', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 161, 'Ilokano_rows': 167, 'aligned_pairs': 167}\n",
      "{'book': '1KI', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 1163, 'Ilokano_rows': 1369, 'aligned_pairs': 1369}\n",
      "{'book': '1MA', 'Pangasinense_files': 1, 'Ilokano_files': 0, 'Pangasinense_rows': 1187, 'Ilokano_rows': 0, 'aligned_pairs': 1187}\n",
      "{'book': '1PE', 'Pangasinense_files': 1, 'Ilokano_files': 1, 'Pangasinense_rows': 159, 'Ilokano_rows': 192, 'aligned_pairs': 192}\n",
      "[1] [1CH] Pangasinense(PNPV_1CH_raw#1): Si Adan so ama nen Set ya ama nen Enos, tan si Enos so ama nen Kenan.  ||  Ilokano(RIPV_1CH_raw#1): Ni Adan ti ama ni Set nga ama ni Enos.\n",
      "[2] [1CH] Pangasinense(PNPV_1CH_raw#2): Si Kenan so ama nen Mahalalel ya ama nen Jared, tan si Jared so ama nen Enoc ya ama nen Matusalem.  ||  Ilokano(RIPV_1CH_raw#2): Ni Enos ti ama ni Kenan.\n",
      "[3] [1CH] Pangasinense(PNPV_1CH_raw#3): Si Matusalem so ama nen Lamec ya ama nen Noe.  ||  Ilokano(RIPV_1CH_raw#3): Ni Kenan ti ama ni Mahalalel nga ama ni Jared.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def _natural_key(s: str):\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
    "\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        nc = c.replace(\"\\ufeff\", \"\").strip().lower() \n",
    "        new_cols[c] = nc\n",
    "    return df.rename(columns=new_cols)\n",
    "\n",
    "def _is_missing_str(s) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    return t == \"\" or t.lower() in {\"nan\", \"none\", \"null\"}\n",
    "\n",
    "def read_csv_sentences(path, keep_empty=False):\n",
    "    \"\"\"\n",
    "    Read a CSV that should contain a single column 'sentence' (case/BOM tolerant).\n",
    "    Returns list[str]. If keep_empty=True, preserves blank lines as \"\".\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    df = None\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=str, encoding=enc, on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to read CSV: {path}\\n{last_err}\")\n",
    "\n",
    "    df = _normalize_columns(df)\n",
    "\n",
    "    if \"sentence\" not in df.columns:\n",
    "        if df.shape[1] == 1:\n",
    "            only = df.columns[0]\n",
    "            df = df.rename(columns={only: \"sentence\"})\n",
    "        else:\n",
    "            raise ValueError(f\"'sentence' column not found in {path}. Columns: {list(df.columns)}\")\n",
    "\n",
    "    s = (\n",
    "        df[\"sentence\"]\n",
    "        .astype(str)\n",
    "        .map(lambda x: x.replace(\"\\r\", \" \").strip())\n",
    "        .tolist()\n",
    "    )\n",
    "    if not keep_empty:\n",
    "        s = [v for v in s if not _is_missing_str(v)]\n",
    "    else:\n",
    "        s = [v if not _is_missing_str(v) else \"\" for v in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "def list_csvs(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively list CSVs under root_dir, natural-sorted for determinism.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(root_dir, \"**\", \"*.csv\")\n",
    "    files = [p for p in glob.glob(pattern, recursive=True) if os.path.isfile(p)]\n",
    "    files.sort(key=_natural_key)\n",
    "    return files\n",
    "\n",
    "def language_name_from_dir(dir_path):\n",
    "    return os.path.basename(os.path.normpath(dir_path)) or \"Language\"\n",
    "\n",
    "def ensure_ext(base, ext):\n",
    "    b, _ = os.path.splitext(base)\n",
    "    return b + ext\n",
    "\n",
    "\n",
    "def write_outputs(df, out_base):\n",
    "    out_csv = ensure_ext(out_base, \".csv\")\n",
    "    out_txt = ensure_ext(out_base, \".txt\")\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, r in df.iterrows():\n",
    "            book = str(r[cols[0]])\n",
    "            l1_id, l1_sent, l2_id, l2_sent = r[cols[1]], r[cols[2]], r[cols[3]], r[cols[4]]\n",
    "\n",
    "            f.write(f\"[{book}] {cols[2]} ({l1_id})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(l1_sent) else str(l1_sent)}\\n\")\n",
    "            f.write(f\"{cols[4]} ({l2_id})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(l2_sent) else str(l2_sent)}\\n\\n\")\n",
    "\n",
    "    return out_csv, out_txt\n",
    "\n",
    "\n",
    "BOOK_RE = re.compile(r'(?<![A-Z0-9])([1-3]?[A-Z]{2,3})(?![A-Z])')\n",
    "\n",
    "def extract_book_code(path):\n",
    "    \"\"\"\n",
    "    Extract the book code strictly as the token between the first and second underscore.\n",
    "    Examples:\n",
    "      'MBB05_MAT_raw_lines.csv'   -> 'MAT'\n",
    "      'VTSP_1CO_raw_lines.csv'    -> '1CO'\n",
    "    Returns None if the filename doesn't have at least two underscores.\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(os.path.basename(path))[0]\n",
    "    parts = base.split('_')\n",
    "    if len(parts) >= 3:             \n",
    "        return parts[1].upper()      \n",
    "    return None\n",
    "\n",
    "def build_book_map(root_dir):\n",
    "    \"\"\"\n",
    "    Returns dict: {book_code: [csv_paths_for_that_book_sorted]}\n",
    "    \"\"\"\n",
    "    book_map = {}\n",
    "    for f in list_csvs(root_dir):\n",
    "        code = extract_book_code(f)\n",
    "        if code:\n",
    "            book_map.setdefault(code, []).append(f)\n",
    "    for k in book_map:\n",
    "        book_map[k].sort(key=_natural_key)\n",
    "    return book_map\n",
    "\n",
    "def read_book_entries(file_list, keep_empty=False):\n",
    "    \"\"\"\n",
    "    For a given book, read all files (in order) and return a list of entries:\n",
    "    [{'id': '<file_stem>#<1based>', 'sentence': '<text>'}, ...]\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    for f in file_list:\n",
    "        stem = os.path.splitext(os.path.basename(f))[0]\n",
    "        sents = read_csv_sentences(f, keep_empty=keep_empty)\n",
    "        for i, sent in enumerate(sents, start=1):\n",
    "            entries.append({\"id\": f\"{stem}#{i}\", \"sentence\": sent})\n",
    "    return entries\n",
    "\n",
    "def merge_pair_by_book(lang1_dir, lang2_dir, out_base_name,\n",
    "                       out_dir=None, use_fixed_headers=False, keep_empty=False):\n",
    "    lang1_dir = os.path.normpath(lang1_dir)\n",
    "    lang2_dir = os.path.normpath(lang2_dir)\n",
    "\n",
    "    l1_name = \"Language 1\" if use_fixed_headers else language_name_from_dir(lang1_dir)\n",
    "    l2_name = \"Language 2\" if use_fixed_headers else language_name_from_dir(lang2_dir)\n",
    "\n",
    "    l1_books = build_book_map(lang1_dir)\n",
    "    l2_books = build_book_map(lang2_dir)\n",
    "\n",
    "    all_books = sorted(set(l1_books.keys()) | set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    rows = []\n",
    "    per_book_stats = []\n",
    "\n",
    "    for bk in all_books:\n",
    "        l1_files = l1_books.get(bk, [])\n",
    "        l2_files = l2_books.get(bk, [])\n",
    "\n",
    "        s1 = read_book_entries(l1_files, keep_empty=keep_empty)\n",
    "        s2 = read_book_entries(l2_files, keep_empty=keep_empty)\n",
    "\n",
    "        filler1 = {\"id\": \"N/A\", \"sentence\": \"N/A\"}\n",
    "        filler2 = {\"id\": \"N/A\", \"sentence\": \"N/A\"}\n",
    "\n",
    "        for e1, e2 in zip_longest(s1, s2, fillvalue=None):\n",
    "            e1 = e1 if e1 is not None else filler1\n",
    "            e2 = e2 if e2 is not None else filler2\n",
    "            rows.append({\n",
    "                \"book\": bk,\n",
    "                f\"{l1_name} ID\": e1[\"id\"],\n",
    "                f\"{l1_name}\": e1[\"sentence\"],\n",
    "                f\"{l2_name} ID\": e2[\"id\"],\n",
    "                f\"{l2_name}\": e2[\"sentence\"],\n",
    "            })\n",
    "\n",
    "        per_book_stats.append({\n",
    "            \"book\": bk,\n",
    "            f\"{l1_name}_files\": len(l1_files),\n",
    "            f\"{l2_name}_files\": len(l2_files),\n",
    "            f\"{l1_name}_rows\": len(s1),\n",
    "            f\"{l2_name}_rows\": len(s2),\n",
    "            \"aligned_pairs\": max(len(s1), len(s2)),  \n",
    "        })\n",
    "\n",
    "    cols = [\"book\", f\"{l1_name} ID\", f\"{l1_name}\", f\"{l2_name} ID\", f\"{l2_name}\"]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    out_base = out_base_name if out_dir is None else os.path.join(out_dir, out_base_name)\n",
    "    out_csv, out_txt = write_outputs(df, out_base)\n",
    "\n",
    "    common_books = sorted(set(l1_books.keys()) & set(l2_books.keys()), key=_natural_key)\n",
    "    missing_l1 = sorted(set(l2_books.keys()) - set(l1_books.keys()), key=_natural_key)\n",
    "    missing_l2 = sorted(set(l1_books.keys()) - set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    print(f\"Books (total): {len(all_books)}; Common books: {len(common_books)}\")\n",
    "    if missing_l1:\n",
    "        print(f\"Books only in {l2_name}: {missing_l1[:10]}{'...' if len(missing_l1)>10 else ''}\")\n",
    "    if missing_l2:\n",
    "        print(f\"Books only in {l1_name}: {missing_l2[:10]}{'...' if len(missing_l2)>10 else ''}\")\n",
    "    print(f\"Total rows (after padding with N/A): {len(df)}\")\n",
    "    print(f\"CSV → {out_csv}\")\n",
    "    print(f\"TXT → {out_txt}\")\n",
    "\n",
    "    print(\"\\nPer-book stats (first 6):\")\n",
    "    for s in per_book_stats[:6]:\n",
    "        print(s)\n",
    "\n",
    "    for i in range(min(3, len(df))):\n",
    "        r = df.iloc[i]\n",
    "        print(f\"[{i+1}] [{r['book']}] {l1_name}({r[f'{l1_name} ID']}): {r[f'{l1_name}']}  ||  \"\n",
    "              f\"{l2_name}({r[f'{l2_name} ID']}): {r[f'{l2_name}']}\")\n",
    "\n",
    "    return out_csv, out_txt, df\n",
    "\n",
    "results = []\n",
    "for pair in PAIRS:\n",
    "    csv_path, txt_path, df_out = merge_pair_by_book(\n",
    "        pair[\"lang1_dir\"], pair[\"lang2_dir\"],\n",
    "        out_base_name=pair[\"out_txt\"],\n",
    "        out_dir=OUT_DIR,\n",
    "        use_fixed_headers=USE_FIXED_HEADERS,\n",
    "        keep_empty=KEEP_EMPTY_LINES,\n",
    "    )\n",
    "    results.append((csv_path, txt_path, len(df_out)))\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
