{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697467f1",
   "metadata": {},
   "source": [
    "# Parallel Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecea2a0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3245aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, Iterable, Union\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from itertools import zip_longest\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391d8fe4",
   "metadata": {},
   "source": [
    "## By verse\n",
    "This script provides a reusable pipeline for building parallel corpora of Bible translations aligned by verse, including cases where verse identifiers are merged across languages. For each configured language pair, it loads all verse-aligned files from two input directories, parses them into dictionaries keyed by USFM identifiers (e.g., 1CO.5.12 or merged keys like 1CO.5.12+1CO.5.13), and automatically harmonizes verse boundaries between languages. When one language merges consecutive verses while the other keeps them separate, the script detects the overlap and merges the corresponding verses to maintain alignment. It then constructs a unified, verse-by-verse (or merged-verse) mapping, fills in missing entries with a placeholder token, and exports two synchronized outputs: a plain-text file with interleaved verse pairs and a CSV file containing structured metadata (book, chapter, verse label, and both language texts). The resulting corpus ensures consistent alignment even when translations differ in verse segmentation, making it suitable for multilingual text mining, translation studies, or machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c56cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_verse.txt\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_verse.txt\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_verse.txt\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/cj/parsed/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_verse.txt\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_verse.txt\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62ddaa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Kapampangan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Bikolano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Tausug True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Yami True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Ilokano True\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8e16520",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 260\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang1_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m lang2_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach pair must include \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang1_dir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang2_dir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_pair\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang1_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang1_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang2_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang2_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_txt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_txt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     all_summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDone processing all pairs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 208\u001b[0m, in \u001b[0;36mprocess_pair\u001b[0;34m(lang1_dir, lang2_dir, out_txt, missing)\u001b[0m\n\u001b[1;32m    206\u001b[0m key, _, _, _ \u001b[38;5;241m=\u001b[39m group_label(atoms)\n\u001b[1;32m    207\u001b[0m t1 \u001b[38;5;241m=\u001b[39m assemble_text_for_group(lang1, atoms, missing)\n\u001b[0;32m--> 208\u001b[0m t2 \u001b[38;5;241m=\u001b[39m \u001b[43massemble_text_for_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m lines_out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m lines_out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 145\u001b[0m, in \u001b[0;36massemble_text_for_group\u001b[0;34m(lang_map, group_atoms, missing)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massemble_text_for_group\u001b[39m(lang_map: Dict[\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    139\u001b[0m                             group_atoms: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    140\u001b[0m                             missing: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    If lang has a matching merged key (any order), use it. Otherwise join member-verse texts.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    When joining, preserve verse order; insert `missing` for absent members.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     sig_to_key \u001b[38;5;241m=\u001b[39m \u001b[43mmap_atoms_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(group_atoms)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m sig_to_key:\n",
      "Cell \u001b[0;32mIn[21], line 133\u001b[0m, in \u001b[0;36mmap_atoms_signature\u001b[0;34m(keys_map)\u001b[0m\n\u001b[1;32m    131\u001b[0m sig \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys_map:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_merged_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    134\u001b[0m         atoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(split_merged(k))\n\u001b[1;32m    135\u001b[0m         sig[atoms] \u001b[38;5;241m=\u001b[39m k\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# helpers\n",
    "\n",
    "DEFAULT_MISSING = '\"N/A\"'\n",
    "MERGE_JOIN_SEP   = \" \"         \n",
    "KEY_JOIN_SEP     = \"+\"        \n",
    "\n",
    "LINE_RE = re.compile(\n",
    "    r'^\\s*(([0-9A-Z]+\\.\\d+\\.\\d+)(?:\\+[0-9A-Z]+\\.\\d+\\.\\d+)*)\\s+(.*\\S)\\s*$'\n",
    ")\n",
    "\n",
    "def _to_path(p):\n",
    "    return None if p is None else (p if isinstance(p, Path) else Path(p))\n",
    "\n",
    "def is_merged_key(usfm: str) -> bool:\n",
    "    return KEY_JOIN_SEP in usfm\n",
    "\n",
    "def split_merged(usfm: str) -> list[str]:\n",
    "    return usfm.split(KEY_JOIN_SEP)\n",
    "\n",
    "def usfm_sort_key(usfm: str):\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if parts else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return (book, chap, verse)\n",
    "\n",
    "def normalize_group_key(atom_ids: list[str]) -> str:\n",
    "    atoms_sorted = sorted(atom_ids, key=usfm_sort_key)\n",
    "    return KEY_JOIN_SEP.join(atoms_sorted)\n",
    "\n",
    "def split_usfm(usfm: str):\n",
    "    parts = usfm.split(\".\")\n",
    "    book = parts[0] if len(parts) > 0 else \"\"\n",
    "    chap = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "    verse = int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 0\n",
    "    return book, chap, verse\n",
    "\n",
    "def _last_dir_name(p: Path) -> str:\n",
    "    return p.name or p.parent.name\n",
    "\n",
    "\n",
    "def parse_txt_file(fp: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse a single .txt file into {usfm_or_merged_key: text}.\n",
    "    Allows merged identifiers joined by '+'.\n",
    "    \"\"\"\n",
    "    d: Dict[str, str] = {}\n",
    "    with fp.open(\"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "        for raw in f:\n",
    "            m = LINE_RE.match(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            key, text = m.group(1), m.group(3).strip()\n",
    "            d[key] = text\n",
    "    return d\n",
    "\n",
    "def load_folder(folder: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Merge all *.txt files in a folder into a single map {key: text}.\n",
    "    Later files overwrite earlier ones on the same key (simple last-wins).\n",
    "    \"\"\"\n",
    "    combined: Dict[str, str] = {}\n",
    "    txts = sorted(folder.glob(\"*.txt\"))\n",
    "    for fp in txts:\n",
    "        part = parse_txt_file(fp)\n",
    "        combined.update(part)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def collect_groups(lang_map: Dict[str, str]) -> list[set[str]]:\n",
    "    \"\"\"Return list of sets of atomic IDs for all merged keys in a language map.\"\"\"\n",
    "    groups = []\n",
    "    for k in lang_map:\n",
    "        if is_merged_key(k):\n",
    "            groups.append(set(split_merged(k)))\n",
    "    return groups\n",
    "\n",
    "def union_overlapping_groups(group_sets: list[set[str]]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Given a list of sets of atomic IDs, union any that overlap.\n",
    "    Returns list of sorted lists of atomic IDs (stable order by USFM).\n",
    "    \"\"\"\n",
    "    groups = [set(g) for g in group_sets]\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        out = []\n",
    "        while groups:\n",
    "            g = groups.pop()\n",
    "            merged = True\n",
    "            while merged:\n",
    "                merged = False\n",
    "                for i in range(len(groups)-1, -1, -1):\n",
    "                    if g & groups[i]:\n",
    "                        g |= groups[i]\n",
    "                        groups.pop(i)\n",
    "                        merged = True\n",
    "                        changed = True\n",
    "            out.append(g)\n",
    "        groups = out\n",
    "    return [sorted(list(g), key=usfm_sort_key) for g in groups]\n",
    "\n",
    "def all_final_groups(lang1: Dict[str,str], lang2: Dict[str,str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Build the final grouping units (lists of atomic IDs) by taking:\n",
    "      - all merged groupings from both langs (unioning overlaps),\n",
    "      - plus singleton groups for any remaining atom not covered.\n",
    "    \"\"\"\n",
    "    g1 = collect_groups(lang1)\n",
    "    g2 = collect_groups(lang2)\n",
    "    merged_groups = union_overlapping_groups(g1 + g2) \n",
    "\n",
    "    covered = set(a for grp in merged_groups for a in grp)\n",
    "\n",
    "    all_atoms = set()\n",
    "    for k in list(lang1.keys()) + list(lang2.keys()):\n",
    "        if is_merged_key(k):\n",
    "            all_atoms.update(split_merged(k))\n",
    "        else:\n",
    "            all_atoms.add(k)\n",
    "\n",
    "    singletons = [[a] for a in sorted(all_atoms - covered, key=usfm_sort_key)]\n",
    "    return merged_groups + singletons\n",
    "\n",
    "def map_atoms_signature(keys_map: Dict[str,str]) -> Dict[frozenset, str]:\n",
    "    \"\"\"\n",
    "    For quick lookup: if a language already has a merged key whose atoms set equals a group,\n",
    "    we want to reuse that exact key string (and its text).\n",
    "    Returns {frozenset(atom_ids): merged_key_string}\n",
    "    \"\"\"\n",
    "    sig = {}\n",
    "    for k in keys_map:\n",
    "        if is_merged_key(k):\n",
    "            atoms = frozenset(split_merged(k))\n",
    "            sig[atoms] = k\n",
    "    return sig\n",
    "\n",
    "def assemble_text_for_group(lang_map: Dict[str,str],\n",
    "                            group_atoms: list[str],\n",
    "                            missing: str) -> str:\n",
    "    \"\"\"\n",
    "    If lang has a matching merged key (any order), use it. Otherwise join member-verse texts.\n",
    "    When joining, preserve verse order; insert `missing` for absent members.\n",
    "    \"\"\"\n",
    "    sig_to_key = map_atoms_signature(lang_map)\n",
    "    sig = frozenset(group_atoms)\n",
    "    if sig in sig_to_key:\n",
    "        return lang_map[sig_to_key[sig]]\n",
    "\n",
    "    parts = []\n",
    "    for a in group_atoms:\n",
    "        parts.append(lang_map.get(a, missing))\n",
    "    return MERGE_JOIN_SEP.join(parts)\n",
    "\n",
    "def group_label(group_atoms: list[str]) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Build output columns for (usfm_key, book, chapter, verse_label) from a group.\n",
    "    - usfm_key: normalized merged key 'a+b+...'\n",
    "    - book, chapter: taken from the first atomic ID (if consistent)\n",
    "    - verse_label: '12+13' if same chapter; otherwise 'mixed'\n",
    "    \"\"\"\n",
    "    atoms_sorted = sorted(group_atoms, key=usfm_sort_key)\n",
    "    key = normalize_group_key(atoms_sorted)\n",
    "\n",
    "    b0, c0, v0 = split_usfm(atoms_sorted[0])\n",
    "    same_book_chapter = all(split_usfm(a)[0] == b0 and split_usfm(a)[1] == c0 for a in atoms_sorted)\n",
    "    if same_book_chapter:\n",
    "        verse_label = KEY_JOIN_SEP.join(str(split_usfm(a)[2]) for a in atoms_sorted)\n",
    "        book, chap = b0, c0\n",
    "    else:\n",
    "        verse_label = \"mixed\"\n",
    "        book, chap = b0, c0  \n",
    "\n",
    "    return key, book, str(chap), verse_label\n",
    "\n",
    "\n",
    "def process_pair(\n",
    "    lang1_dir: Path,\n",
    "    lang2_dir: Path,\n",
    "    out_txt: Path,\n",
    "    missing: str = DEFAULT_MISSING,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process one language pair and write:\n",
    "      - TXT with stacked lines per GROUP-KEY:  \"<KEY> <lang1_text>\" then \"<KEY> <lang2_text>\"\n",
    "      - CSV with columns: usfm_key, book, verse(s), chapter, language 1, language2\n",
    "    \"\"\"\n",
    "    assert lang1_dir.is_dir(), f\"Not a directory: {lang1_dir}\"\n",
    "    assert lang2_dir.is_dir(), f\"Not a directory: {lang2_dir}\"\n",
    "\n",
    "    if out_txt is None:\n",
    "        name1 = _last_dir_name(lang1_dir)\n",
    "        name2 = _last_dir_name(lang2_dir)\n",
    "        out_txt = Path(f\"{name1}_{name2}_verse.txt\")\n",
    "    out_csv = out_txt.with_suffix(\".csv\")\n",
    "\n",
    "    lang1 = load_folder(lang1_dir)\n",
    "    lang2 = load_folder(lang2_dir)\n",
    "\n",
    "    groups = all_final_groups(lang1, lang2)\n",
    "\n",
    "    groups.sort(key=lambda g: usfm_sort_key(sorted(g, key=usfm_sort_key)[0]))\n",
    "\n",
    "    lines_out = []\n",
    "    for atoms in groups:\n",
    "        key, _, _, _ = group_label(atoms)\n",
    "        t1 = assemble_text_for_group(lang1, atoms, missing)\n",
    "        t2 = assemble_text_for_group(lang2, atoms, missing)\n",
    "        lines_out.append(f\"{key} {t1}\")\n",
    "        lines_out.append(f\"{key} {t2}\")\n",
    "    out_txt.write_text(\"\\n\".join(lines_out) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "    with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"usfm\", \"book\", \"verse\", \"chapter\", \"language 1\", \"language2\"])\n",
    "        for atoms in groups:\n",
    "            key, book, chap, verse_label = group_label(atoms)\n",
    "            t1 = assemble_text_for_group(lang1, atoms, missing)\n",
    "            t2 = assemble_text_for_group(lang2, atoms, missing)\n",
    "            w.writerow([key, book, verse_label, chap, t1, t2])\n",
    "\n",
    "    all_atoms_lang1 = set()\n",
    "    for k in lang1:\n",
    "        all_atoms_lang1.update(split_merged(k) if is_merged_key(k) else [k])\n",
    "    all_atoms_lang2 = set()\n",
    "    for k in lang2:\n",
    "        all_atoms_lang2.update(split_merged(k) if is_merged_key(k) else [k])\n",
    "\n",
    "    group_keys = [normalize_group_key(g) for g in groups]\n",
    "    summary = {\n",
    "        \"out_txt\": str(out_txt),\n",
    "        \"out_csv\": str(out_csv),\n",
    "        \"txt_lines_written\": len(lines_out),\n",
    "        \"csv_rows_written\": len(groups) + 1,  # +1 for header\n",
    "        \"groups_total\": len(groups),\n",
    "        \"missing_in_lang1_atoms\": sum(1 for g in groups for a in g if a not in all_atoms_lang1),\n",
    "        \"missing_in_lang2_atoms\": sum(1 for g in groups for a in g if a not in all_atoms_lang2),\n",
    "        \"missing_token\": missing,\n",
    "        \"merge_join_sep\": MERGE_JOIN_SEP,\n",
    "        \"key_join_sep\": KEY_JOIN_SEP,\n",
    "    }\n",
    "    print(\n",
    "        f\"Processed {out_txt.name} & {out_csv.name} | groups: {summary['groups_total']} | \"\n",
    "        f\"missing_atoms(lang1): {summary['missing_in_lang1_atoms']} | missing_atoms(lang2): {summary['missing_in_lang2_atoms']}\"\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "# main\n",
    "all_summaries = []\n",
    "for cfg in PAIRS:\n",
    "    lang1_dir = _to_path(cfg.get(\"lang1_dir\"))\n",
    "    lang2_dir = _to_path(cfg.get(\"lang2_dir\"))\n",
    "    out_txt   = _to_path(cfg.get(\"out_txt\"))\n",
    "    missing   = cfg.get(\"missing\", DEFAULT_MISSING)\n",
    "\n",
    "    if lang1_dir is None or lang2_dir is None:\n",
    "        raise ValueError(\"Each pair must include 'lang1_dir' and 'lang2_dir'.\")\n",
    "\n",
    "    summary = process_pair(\n",
    "        lang1_dir=lang1_dir,\n",
    "        lang2_dir=lang2_dir,\n",
    "        out_txt=out_txt,\n",
    "        missing=missing,\n",
    "    )\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "print(\"\\nDone processing all pairs.\")\n",
    "for s in all_summaries:\n",
    "    print(f\"- Wrote TXT: {s['out_txt']} | CSV: {s['out_csv']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e76df",
   "metadata": {},
   "source": [
    "## By sentence\n",
    "\n",
    "This code builds parallel corpora for multiple language pairs by aligning sentence-level files book by book.\n",
    "\n",
    "- Natural-sorted, deterministic traversal of files.\n",
    "- Book detection from filename (token between the first and second underscore).\n",
    "- Verse-aware grouping with merged verse ID normalization (e.g., \"1CO.5.12+1CO.5.13\" -> \"1CO.5.12-13\").\n",
    "- Monotone dynamic program (DP) for sentence alignment within each verse span:\n",
    "  supports 1-1, 1-2, 2-1, and 2-2 merges.\n",
    "- TXT output remains human-readable; CSV keeps structured IDs and text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd694bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE: Language pairs\n",
    "PAIRS = [\n",
    "    {   # Tagalog-Kapampangan\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Kapampangan\",\n",
    "        \"out_txt\": \"tagalog_kapampangan_sentence\",\n",
    "    },\n",
    "    {   # Tagalog-Bikolano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Tagalog\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Bikolano\",\n",
    "        \"out_txt\": \"tagalog_bikolano_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"cebuano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Cebuano-Tausug\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Cebuano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Tausug\",\n",
    "        \"out_txt\": \"cebuano_tausug_sentence\",\n",
    "    },\n",
    "    {   # Chavacano-Spanish\n",
    "        \"lang1_dir\": \"../parser/yna/parsed/Chavacano\",\n",
    "        \"lang2_dir\": \"../parser/yna/parsed/Spanish\",\n",
    "        \"out_txt\": \"chavacano_spanish_sentence\",\n",
    "    },\n",
    "    {   # Ivatan-Yami\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Ivatan\",\n",
    "        \"lang2_dir\": \"../parser/cj/parsed/Yami\",\n",
    "        \"out_txt\": \"ivatan_yami_sentence\",\n",
    "    },\n",
    "    {   # Pangasinene-Ilokano\n",
    "        \"lang1_dir\": \"../parser/cj/parsed/Pangasinense\",\n",
    "        \"lang2_dir\": \"../parser/trish/parsed/Ilokano\",\n",
    "        \"out_txt\": \"pangasinense_ilokano_sentence\",\n",
    "    },\n",
    "]\n",
    "\n",
    "OUT_DIR = None\n",
    "USE_FIXED_HEADERS = False\n",
    "KEEP_EMPTY_LINES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b80bccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Kapampangan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Tagalog True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Bikolano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Cebuano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Tausug True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Chavacano True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/yna/parsed/Spanish True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Ivatan True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Yami True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/cj/parsed/Pangasinense True\n",
      "/Users/armina/Documents/GitHub/bible-dot-com-scraper/parser/trish/parsed/Ilokano True\n"
     ]
    }
   ],
   "source": [
    "for pair in PAIRS:\n",
    "    print(Path(pair[\"lang1_dir\"]).resolve(), Path(pair[\"lang1_dir\"]).is_dir())\n",
    "    print(Path(pair[\"lang2_dir\"]).resolve(), Path(pair[\"lang2_dir\"]).is_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books (total): 77; Common: 77\n",
      "Total aligned units (rows): 35515\n",
      "CSV → tagalog_kapampangan_sentence.csv\n",
      "TXT → tagalog_kapampangan_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_rows': 867, 'Kapampangan_rows': 942, 'aligned_units': 954}\n",
      "{'book': '1CO', 'Tagalog_rows': 436, 'Kapampangan_rows': 436, 'aligned_units': 436}\n",
      "{'book': '1JN', 'Tagalog_rows': 104, 'Kapampangan_rows': 105, 'aligned_units': 106}\n",
      "{'book': '1KI', 'Tagalog_rows': 810, 'Kapampangan_rows': 815, 'aligned_units': 819}\n",
      "{'book': '1MA', 'Tagalog_rows': 921, 'Kapampangan_rows': 924, 'aligned_units': 927}\n",
      "{'book': '1PE', 'Tagalog_rows': 105, 'Kapampangan_rows': 105, 'aligned_units': 105}\n",
      "[1] [1CH 1CH.1.1] Tagalog(144): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Kapampangan(1141): Anak neng Adan i Set, at anak neng Set i Enos.\n",
      "[2] [1CH 1CH.1.2] Tagalog(144): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Kapampangan(1141): Anak neng Enos i Kenan, at anak neng Kenan i Mahalalel. Anak neng Mahalalel i Jared.\n",
      "[3] [1CH 1CH.1.3] Tagalog(144): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Kapampangan(1141): Anak neng Jared i Enoc, at anak neng Enoc i Matusalem. Anak neng Matusalem i Lamec.\n",
      "Books (total): 77; Common: 77\n",
      "Total aligned units (rows): 35260\n",
      "CSV → tagalog_bikolano_sentence.csv\n",
      "TXT → tagalog_bikolano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Tagalog_rows': 867, 'Bikolano_rows': 816, 'aligned_units': 903}\n",
      "{'book': '1CO', 'Tagalog_rows': 436, 'Bikolano_rows': 436, 'aligned_units': 436}\n",
      "{'book': '1JN', 'Tagalog_rows': 104, 'Bikolano_rows': 105, 'aligned_units': 106}\n",
      "{'book': '1KI', 'Tagalog_rows': 810, 'Bikolano_rows': 804, 'aligned_units': 822}\n",
      "{'book': '1MA', 'Tagalog_rows': 921, 'Bikolano_rows': 914, 'aligned_units': 928}\n",
      "{'book': '1PE', 'Tagalog_rows': 105, 'Bikolano_rows': 105, 'aligned_units': 105}\n",
      "[1] [1CH 1CH.1.1] Tagalog(144): Si Adan ang ama ni Set at si Set ang ama ni Enos.  ||  Bikolano(890): Si Adan iyo an ama ni Set, asin si Set iyo an ama ni Enos na ama ni Kenan.\n",
      "[2] [1CH 1CH.1.2] Tagalog(144): Si Enos ang ama ni Kenan at si Kenan ang ama ni Mahalalel na ama ni Jared.  ||  Bikolano(890): Si Kenan iyo an ama ni Mahalalel na ama ni Jared.\n",
      "[3] [1CH 1CH.1.3] Tagalog(144): Si Jared ang ama ni Enoc, at anak ni Enoc si Matusalem na ama ni Lamec.  ||  Bikolano(890): Si Jared iyo an ama ni Enoc na ama ni Metusela; si Metusela iyo an ama ni Lamec\n",
      "Books (total): 66; Common: 66\n",
      "Total aligned units (rows): 31105\n",
      "CSV → cebuano_spanish_sentence.csv\n",
      "TXT → cebuano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_rows': 942, 'Spanish_rows': 942, 'aligned_units': 942}\n",
      "{'book': '1CO', 'Cebuano_rows': 437, 'Spanish_rows': 437, 'aligned_units': 437}\n",
      "{'book': '1JN', 'Cebuano_rows': 105, 'Spanish_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1KI', 'Cebuano_rows': 816, 'Spanish_rows': 816, 'aligned_units': 816}\n",
      "{'book': '1PE', 'Cebuano_rows': 105, 'Spanish_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1SA', 'Cebuano_rows': 810, 'Spanish_rows': 810, 'aligned_units': 810}\n",
      "[1] [1CH 1CH.1.1] Cebuano(2187): Si Adan, si Set, si Enos,  ||  Spanish(150): Adán, Set, Enós,\n",
      "[2] [1CH 1CH.1.2] Cebuano(2187): si Kenan, si Mahalalel, si Jared,  ||  Spanish(150): Cainán, Mahalaleel, Jared,\n",
      "[3] [1CH 1CH.1.3] Cebuano(2187): si Enoc, si Metusela, si Lamec,  ||  Spanish(150): Enoc, Matusalén, Lamec,\n",
      "Books (total): 66; Common: 27\n",
      "Books only in Cebuano: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total aligned units (rows): 31146\n",
      "CSV → cebuano_tausug_sentence.csv\n",
      "TXT → cebuano_tausug_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Cebuano_rows': 942, 'Tausug_rows': 0, 'aligned_units': 942}\n",
      "{'book': '1CO', 'Cebuano_rows': 437, 'Tausug_rows': 434, 'aligned_units': 440}\n",
      "{'book': '1JN', 'Cebuano_rows': 105, 'Tausug_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1KI', 'Cebuano_rows': 816, 'Tausug_rows': 0, 'aligned_units': 816}\n",
      "{'book': '1PE', 'Cebuano_rows': 105, 'Tausug_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1SA', 'Cebuano_rows': 810, 'Tausug_rows': 0, 'aligned_units': 810}\n",
      "[1] [1CH 1CH.1.1] Cebuano(2187): Si Adan, si Set, si Enos,  ||  Tausug(N/A): N/A\n",
      "[2] [1CH 1CH.1.2] Cebuano(2187): si Kenan, si Mahalalel, si Jared,  ||  Tausug(N/A): N/A\n",
      "[3] [1CH 1CH.1.3] Cebuano(2187): si Enoc, si Metusela, si Lamec,  ||  Tausug(N/A): N/A\n",
      "Books (total): 66; Common: 27\n",
      "Books only in Spanish: ['1CH', '1KI', '1SA', '2CH', '2KI', '2SA', 'AMO', 'DAN', 'DEU', 'ECC']...\n",
      "Total aligned units (rows): 31120\n",
      "CSV → chavacano_spanish_sentence.csv\n",
      "TXT → chavacano_spanish_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Chavacano_rows': 0, 'Spanish_rows': 942, 'aligned_units': 942}\n",
      "{'book': '1CO', 'Chavacano_rows': 435, 'Spanish_rows': 437, 'aligned_units': 439}\n",
      "{'book': '1JN', 'Chavacano_rows': 104, 'Spanish_rows': 105, 'aligned_units': 106}\n",
      "{'book': '1KI', 'Chavacano_rows': 0, 'Spanish_rows': 816, 'aligned_units': 816}\n",
      "{'book': '1PE', 'Chavacano_rows': 105, 'Spanish_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1SA', 'Chavacano_rows': 0, 'Spanish_rows': 810, 'aligned_units': 810}\n",
      "[1] [1CH 1CH.1.1] Chavacano(N/A): N/A  ||  Spanish(150): Adán, Set, Enós,\n",
      "[2] [1CH 1CH.1.2] Chavacano(N/A): N/A  ||  Spanish(150): Cainán, Mahalaleel, Jared,\n",
      "[3] [1CH 1CH.1.3] Chavacano(N/A): N/A  ||  Spanish(150): Enoc, Matusalén, Lamec,\n",
      "Books (total): 29; Common: 27\n",
      "Books only in Ivatan: ['PRO', 'PSA']\n",
      "Total aligned units (rows): 9606\n",
      "CSV → ivatan_yami_sentence.csv\n",
      "TXT → ivatan_yami_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CO', 'Ivatan_rows': 436, 'Yami_rows': 433, 'aligned_units': 439}\n",
      "{'book': '1JN', 'Ivatan_rows': 105, 'Yami_rows': 105, 'aligned_units': 105}\n",
      "{'book': '1PE', 'Ivatan_rows': 105, 'Yami_rows': 104, 'aligned_units': 106}\n",
      "{'book': '1TH', 'Ivatan_rows': 89, 'Yami_rows': 89, 'aligned_units': 89}\n",
      "{'book': '1TI', 'Ivatan_rows': 113, 'Yami_rows': 113, 'aligned_units': 113}\n",
      "{'book': '2CO', 'Ivatan_rows': 257, 'Yami_rows': 256, 'aligned_units': 258}\n",
      "[1] [1CO 1CO.1.1] Ivatan(1315): Si Pablo ako a tinawagan no Dios a asa ka apostol ni Jesu Cristo a makayamot do inolay na as kani Sostenes a kakteh ta.  ||  Yami(N/A): N/A\n",
      "[2] [1CO 1CO.1.1-2] Ivatan(N/A): N/A  ||  Yami(2364): Yaken rana am si Pawlo ko a yana nipamnekan ni Yeso Kizisto a amlivolivon no ciriciring na a lamlamsoy na ni Ama ta do to. Yaken rana kano akma tey kakteh do cinai si Sotenay am namen ipeyteygami inyo a sinjya do Kedinto a kyokay. Inyo rana kano yamakakaday do peycyeylilyan na a macikaop ji Kizisto a tao am yatamo meyyangangay a na nipamnekan ni Ama ta do to a italamozong na. Si Kizisto am Panirsirngen da. So yata icyakman sang a yamapeypanirsirngen sya.\n",
      "[3] [1CO 1CO.1.2] Ivatan(1315): Tayto kami a maytolas dinio do timban aya no Dios do Corinto, dinio a pinayvadiw na a manamonamo a makayamot di Jesu Cristo as tinawagan na a tawotawo na a akma sira so kadwan do matatarek a logar a tayto a omanianib so Apo taya a si Jesu Cristo a iya so Apohen ta atavo.  ||  Yami(N/A): N/A\n",
      "Books (total): 77; Common: 66\n",
      "Books only in Pangasinense: ['1MA', '2MA', 'BAR', 'BEL', 'JDT', 'LJE', 'S3Y', 'SIR', 'SUS', 'TOB']...\n",
      "Total aligned units (rows): 34609\n",
      "CSV → pangasinense_ilokano_sentence.csv\n",
      "TXT → pangasinense_ilokano_sentence.txt\n",
      "\n",
      "Per-book stats (first 6):\n",
      "{'book': '1CH', 'Pangasinense_rows': 818, 'Ilokano_rows': 799, 'aligned_units': 827}\n",
      "{'book': '1CO', 'Pangasinense_rows': 436, 'Ilokano_rows': 433, 'aligned_units': 438}\n",
      "{'book': '1JN', 'Pangasinense_rows': 105, 'Ilokano_rows': 103, 'aligned_units': 107}\n",
      "{'book': '1KI', 'Pangasinense_rows': 801, 'Ilokano_rows': 800, 'aligned_units': 802}\n",
      "{'book': '1MA', 'Pangasinense_rows': 914, 'Ilokano_rows': 0, 'aligned_units': 914}\n",
      "{'book': '1PE', 'Pangasinense_rows': 105, 'Ilokano_rows': 103, 'aligned_units': 107}\n",
      "[1] [1CH 1CH.1.1] Pangasinense(1166): Si Adan so ama nen Set ya ama nen Enos, tan si Enos so ama nen Kenan.  ||  Ilokano(782): Ni Adan ti ama ni Set nga ama ni Enos. Ni Enos ti ama ni Kenan.\n",
      "[2] [1CH 1CH.1.2] Pangasinense(1166): Si Kenan so ama nen Mahalalel ya ama nen Jared,  ||  Ilokano(782): Ni Kenan ti ama ni Mahalalel nga ama ni Jared.\n",
      "[3] [1CH 1CH.1.3] Pangasinense(1166): tan si Jared so ama nen Enoc ya ama nen Matusalem. Si Matusalem so ama nen Lamec  ||  Ilokano(782): Ni Jared ti ama ni Enoc nga ama ni Matusalem nga ama ni Lamec.\n",
      "\n",
      "Done.\n",
      "Summary:\n",
      "  tagalog_kapampangan_sentence.csv | rows=35515\n",
      "  tagalog_bikolano_sentence.csv | rows=35260\n",
      "  cebuano_spanish_sentence.csv | rows=31105\n",
      "  cebuano_tausug_sentence.csv | rows=31146\n",
      "  chavacano_spanish_sentence.csv | rows=31120\n",
      "  ivatan_yami_sentence.csv | rows=9606\n",
      "  pangasinense_ilokano_sentence.csv | rows=34609\n"
     ]
    }
   ],
   "source": [
    "# helpers\n",
    "\n",
    "\n",
    "def _natural_key(s: str):\n",
    "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        nc = c.replace(\"\\ufeff\", \"\").strip().lower()\n",
    "        new_cols[c] = nc\n",
    "    return df.rename(columns=new_cols)\n",
    "\n",
    "def _is_missing_str(s) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    return t == \"\" or t.lower() in {\"nan\", \"none\", \"null\"}\n",
    "\n",
    "def list_csvs(root_dir):\n",
    "    pattern = os.path.join(root_dir, \"**\", \"*.csv\")\n",
    "    files = [p for p in glob.glob(pattern, recursive=True) if os.path.isfile(p)]\n",
    "    files.sort(key=_natural_key)\n",
    "    return files\n",
    "\n",
    "def language_name_from_dir(dir_path):\n",
    "    return os.path.basename(os.path.normpath(dir_path)) or \"Language\"\n",
    "\n",
    "def ensure_ext(base, ext):\n",
    "    b, _ = os.path.splitext(base)\n",
    "    return b + ext\n",
    "\n",
    "def write_outputs(df, out_base, l1_name, l2_name):\n",
    "    out_csv = ensure_ext(out_base, \".csv\")\n",
    "    out_txt = ensure_ext(out_base, \".txt\")\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, r in df.iterrows():\n",
    "            f.write(f\"[{r['book']}] {r['verse']}\\n\")\n",
    "            f.write(f\"{l1_name} ({r[f'{l1_name} ID']})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(r[f'{l1_name}']) else str(r[f'{l1_name}'])}\\n\")\n",
    "            f.write(f\"{l2_name} ({r[f'{l2_name} ID']})\\n\")\n",
    "            f.write(f\"{'' if pd.isna(r[f'{l2_name}']) else str(r[f'{l2_name}'])}\\n\\n\")\n",
    "\n",
    "    return out_csv, out_txt\n",
    "\n",
    "def read_records_usfm_schema(path, keep_empty=False):\n",
    "    \"\"\"\n",
    "    Reads a CSV with columns (case-insensitive):\n",
    "      usfm, book, chapter, verse, text, iso6393, vid\n",
    "    Returns list of dicts per row: {'usfm','book','chapter','verse','text','iso6393','vid'}\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    df = None\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=str, encoding=enc, on_bad_lines=\"skip\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"Failed to read CSV: {path}\\n{last_err}\")\n",
    "\n",
    "    df = _normalize_columns(df)\n",
    "\n",
    "    required = {\"usfm\", \"book\", \"chapter\", \"verse\"}\n",
    "    has_text = \"text\" in df.columns or \"sentence\" in df.columns\n",
    "    missing = required - set(df.columns)\n",
    "    if missing or not has_text:\n",
    "        raise ValueError(f\"CSV {path} missing columns. Needed {required} + text/sentence. Got: {list(df.columns)}\")\n",
    "\n",
    "    text_col = \"text\" if \"text\" in df.columns else \"sentence\"\n",
    "\n",
    "    tser = df[text_col].astype(str).map(lambda x: x.replace(\"\\r\", \" \").strip())\n",
    "    if not keep_empty:\n",
    "        mask = ~tser.map(_is_missing_str)\n",
    "        df = df[mask].copy()\n",
    "        tser = tser[mask]\n",
    "\n",
    "    df[text_col] = np.where(tser.map(_is_missing_str), \"\", tser)\n",
    "\n",
    "    for opt in [\"iso6393\", \"vid\"]:\n",
    "        if opt not in df.columns:\n",
    "            df[opt] = \"\"\n",
    "\n",
    "    rows = []\n",
    "    stem = os.path.splitext(os.path.basename(path))[0]\n",
    "    for i, r in df.iterrows():\n",
    "        vid = (str(r[\"vid\"]).strip() if str(r[\"vid\"]).strip() else f\"{stem}#{i+1}\")\n",
    "        rows.append({\n",
    "            \"usfm\": str(r[\"usfm\"]).strip(),\n",
    "            \"book\": str(r[\"book\"]).strip(),\n",
    "            \"chapter\": str(r[\"chapter\"]).strip(),\n",
    "            \"verse\": str(r[\"verse\"]).strip(),\n",
    "            \"text\": str(r[text_col]),\n",
    "            \"iso6393\": str(r[\"iso6393\"]).strip(),\n",
    "            \"vid\": vid,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# verse span handling\n",
    "\n",
    "\n",
    "MERGE_SEP = \"+\"\n",
    "\n",
    "def _normalize_verse_span(usfm_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Turn 'BOOK.C.V1+BOOK.C.V2+...' -> 'BOOK.C.Vmin-Vmax'\n",
    "    Single IDs pass through unchanged.\n",
    "    \"\"\"\n",
    "    if MERGE_SEP not in usfm_id:\n",
    "        return usfm_id\n",
    "    parts = [p.strip() for p in usfm_id.split(MERGE_SEP) if p.strip()]\n",
    "    book_ch = \".\".join(parts[0].split(\".\")[:2])  \n",
    "    verses = []\n",
    "    for p in parts:\n",
    "        toks = p.split(\".\")\n",
    "        try:\n",
    "            verses.append(int(toks[-1]))\n",
    "        except Exception:\n",
    "            v = re.sub(r\"\\D+\", \"\", toks[-1])\n",
    "            verses.append(int(v) if v.isdigit() else 0)\n",
    "    vmin, vmax = min(verses), max(verses)\n",
    "    return f\"{book_ch}.{vmin}-{vmax}\"\n",
    "\n",
    "\n",
    "def read_dir_grouped_by_bookverse(root_dir, keep_empty=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      books: dict[book_code] -> dict[verse_span_id] -> list[{id:'vid', text:'...'}]\n",
    "    It reads every CSV and buckets rows by (book, normalized usfm span).\n",
    "    \"\"\"\n",
    "    books = {}\n",
    "    for path in list_csvs(root_dir):\n",
    "        try:\n",
    "            recs = read_records_usfm_schema(path, keep_empty=keep_empty)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {path}: {e}\")\n",
    "            continue\n",
    "        for r in recs:\n",
    "            book = r[\"book\"].upper()\n",
    "            span = _normalize_verse_span(r[\"usfm\"])\n",
    "            books.setdefault(book, {}).setdefault(span, []).append({\"id\": r[\"vid\"], \"text\": r[\"text\"]})\n",
    "    return books\n",
    "\n",
    "NUM_RE = re.compile(r'\\d+')\n",
    "\n",
    "def _len_sim(a: str, b: str) -> float:\n",
    "    la, lb = max(1, len(a)), max(1, len(b))\n",
    "    return 1.0 - abs(la - lb) / max(la, lb)\n",
    "\n",
    "def _anchors(a: str, b: str) -> float:\n",
    "    bonus = 0.0\n",
    "    nums_a = set(NUM_RE.findall(a))\n",
    "    nums_b = set(NUM_RE.findall(b))\n",
    "    if nums_a & nums_b:\n",
    "        bonus += 0.15\n",
    "    names_a = set(t for t in re.findall(r'\\b[A-Z][A-Za-z]+\\b', a))\n",
    "    names_b = set(t for t in re.findall(r'\\b[A-Z][A-Za-z]+\\b', b))\n",
    "    if names_a and names_b and (names_a & names_b):\n",
    "        bonus += 0.1\n",
    "    return bonus\n",
    "\n",
    "def _score_block(sa: list[str], tb: list[str], merge_penalty: float) -> float:\n",
    "    a = \" \".join(sa).strip()\n",
    "    b = \" \".join(tb).strip()\n",
    "    base = _len_sim(a, b) + _anchors(a, b)\n",
    "    merges = (len(sa) - 1) + (len(tb) - 1)\n",
    "    return base - merge_penalty * merges\n",
    "\n",
    "def align_within_verse(sent_a: list[str], sent_b: list[str], merge_penalty: float = 0.15):\n",
    "    \"\"\"\n",
    "    Returns list of tuples: (a_start,a_end, b_start,b_end, score)\n",
    "    half-open ranges; supports 1-1, 1-2, 2-1, 2-2; monotone, no crossings.\n",
    "    \"\"\"\n",
    "    m, n = len(sent_a), len(sent_b)\n",
    "    INF = -1e9\n",
    "    dp = [[INF]*(n+1) for _ in range(m+1)]\n",
    "    back = [[None]*(n+1) for _ in range(m+1)]\n",
    "    dp[0][0] = 0.0\n",
    "\n",
    "    def S(i,j,ii,jj):\n",
    "        return _score_block(sent_a[i:ii], sent_b[j:jj], merge_penalty)\n",
    "\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            cur = dp[i][j]\n",
    "            if cur <= INF/2:\n",
    "                continue\n",
    "            if i < m and j < n:\n",
    "                sc = cur + S(i,j,i+1,j+1)\n",
    "                if sc > dp[i+1][j+1]:\n",
    "                    dp[i+1][j+1] = sc; back[i+1][j+1] = (i,j, i+1,j+1)\n",
    "            if i < m and j+1 < n:\n",
    "                sc = cur + S(i,j,i+1,j+2)\n",
    "                if sc > dp[i+1][j+2]:\n",
    "                    dp[i+1][j+2] = sc; back[i+1][j+2] = (i,j, i+1,j+2)\n",
    "            if i+1 < m and j < n:\n",
    "                sc = cur + S(i,j,i+2,j+1)\n",
    "                if sc > dp[i+2][j+1]:\n",
    "                    dp[i+2][j+1] = sc; back[i+2][j+1] = (i,j, i+2,j+1)\n",
    "            if i+1 < m and j+1 < n:\n",
    "                sc = cur + S(i,j,i+2,j+2)\n",
    "                if sc > dp[i+2][j+2]:\n",
    "                    dp[i+2][j+2] = sc; back[i+2][j+2] = (i,j, i+2,j+2)\n",
    "\n",
    "    i, j = m, n\n",
    "    path = []\n",
    "    while i > 0 or j > 0:\n",
    "        if back[i][j] is None:\n",
    "            if i > 0:\n",
    "                path.append((i-1, i, j, j, _score_block([sent_a[i-1]], [], merge_penalty)))\n",
    "                i -= 1\n",
    "            elif j > 0:\n",
    "                path.append((i, i, j-1, j, _score_block([], [sent_b[j-1]], merge_penalty)))\n",
    "                j -= 1\n",
    "        else:\n",
    "            pi, pj, ii, jj = back[i][j]\n",
    "            path.append((pi, ii, pj, jj, _score_block(sent_a[pi:ii], sent_b[pj:jj], merge_penalty)))\n",
    "            i, j = pi, pj\n",
    "    path.reverse()\n",
    "    return path\n",
    "\n",
    "# merge: book → verse → aligned rows\n",
    "\n",
    "def merge_pair_by_verse_dirs(lang1_dir, lang2_dir, out_base_name,\n",
    "                             out_dir=None, use_fixed_headers=False, keep_empty=False):\n",
    "    lang1_dir = os.path.normpath(lang1_dir)\n",
    "    lang2_dir = os.path.normpath(lang2_dir)\n",
    "\n",
    "    l1_name = \"Language 1\" if use_fixed_headers else language_name_from_dir(lang1_dir)\n",
    "    l2_name = \"Language 2\" if use_fixed_headers else language_name_from_dir(lang2_dir)\n",
    "\n",
    "    l1_books = read_dir_grouped_by_bookverse(lang1_dir, keep_empty=keep_empty)\n",
    "    l2_books = read_dir_grouped_by_bookverse(lang2_dir, keep_empty=keep_empty)\n",
    "\n",
    "    all_books = sorted(set(l1_books.keys()) | set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    rows, per_book_stats = [], []\n",
    "\n",
    "    for bk in all_books:\n",
    "        v1 = l1_books.get(bk, {})\n",
    "        v2 = l2_books.get(bk, {})\n",
    "\n",
    "        all_spans = sorted(set(v1.keys()) | set(v2.keys()), key=_natural_key)\n",
    "\n",
    "        aligned_units = 0\n",
    "        l1_rows_total = sum(len(v) for v in v1.values())\n",
    "        l2_rows_total = sum(len(v) for v in v2.values())\n",
    "\n",
    "        for span in all_spans:\n",
    "            a_ids = [e[\"id\"] for e in v1.get(span, [])]\n",
    "            b_ids = [e[\"id\"] for e in v2.get(span, [])]\n",
    "            a = [e[\"text\"] for e in v1.get(span, [])]\n",
    "            b = [e[\"text\"] for e in v2.get(span, [])]\n",
    "\n",
    "            if a and b:\n",
    "                path = align_within_verse(a, b)\n",
    "                for (i0, i1, j0, j1, _sc) in path:\n",
    "                    l1_text = \" \".join(a[i0:i1]) if i0 < i1 else \"N/A\"\n",
    "                    l2_text = \" \".join(b[j0:j1]) if j0 < j1 else \"N/A\"\n",
    "                    l1_id = \",\".join(a_ids[i0:i1]) if i0 < i1 else \"N/A\"\n",
    "                    l2_id = \",\".join(b_ids[j0:j1]) if j0 < j1 else \"N/A\"\n",
    "                    rows.append({\n",
    "                        \"book\": bk, \"verse\": span,\n",
    "                        f\"{l1_name} ID\": l1_id, f\"{l1_name}\": l1_text,\n",
    "                        f\"{l2_name} ID\": l2_id, f\"{l2_name}\": l2_text,\n",
    "                    })\n",
    "                    aligned_units += 1\n",
    "            elif a and not b:\n",
    "                for i in range(len(a)):\n",
    "                    rows.append({\n",
    "                        \"book\": bk, \"verse\": span,\n",
    "                        f\"{l1_name} ID\": a_ids[i], f\"{l1_name}\": a[i],\n",
    "                        f\"{l2_name} ID\": \"N/A\", f\"{l2_name}\": \"N/A\",\n",
    "                    })\n",
    "                    aligned_units += 1\n",
    "            elif b and not a:\n",
    "                for j in range(len(b)):\n",
    "                    rows.append({\n",
    "                        \"book\": bk, \"verse\": span,\n",
    "                        f\"{l1_name} ID\": \"N/A\", f\"{l1_name}\": \"N/A\",\n",
    "                        f\"{l2_name} ID\": b_ids[j], f\"{l2_name}\": b[j],\n",
    "                    })\n",
    "                    aligned_units += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        per_book_stats.append({\n",
    "            \"book\": bk,\n",
    "            f\"{l1_name}_rows\": l1_rows_total,\n",
    "            f\"{l2_name}_rows\": l2_rows_total,\n",
    "            \"aligned_units\": aligned_units,\n",
    "        })\n",
    "\n",
    "    cols = [\"book\", \"verse\", f\"{l1_name} ID\", f\"{l1_name}\", f\"{l2_name} ID\", f\"{l2_name}\"]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    out_base = out_base_name if out_dir is None else os.path.join(out_dir, out_base_name)\n",
    "    out_csv, out_txt = write_outputs(df, out_base, l1_name, l2_name)\n",
    "\n",
    "    common_books = sorted(set(l1_books.keys()) & set(l2_books.keys()), key=_natural_key)\n",
    "    missing_l1 = sorted(set(l2_books.keys()) - set(l1_books.keys()), key=_natural_key)\n",
    "    missing_l2 = sorted(set(l1_books.keys()) - set(l2_books.keys()), key=_natural_key)\n",
    "\n",
    "    print(f\"Books (total): {len(all_books)}; Common: {len(common_books)}\")\n",
    "    if missing_l1:\n",
    "        print(f\"Books only in {l2_name}: {missing_l1[:10]}{'...' if len(missing_l1)>10 else ''}\")\n",
    "    if missing_l2:\n",
    "        print(f\"Books only in {l1_name}: {missing_l2[:10]}{'...' if len(missing_l2)>10 else ''}\")\n",
    "    print(f\"Total aligned units (rows): {len(df)}\")\n",
    "    print(f\"CSV → {out_csv}\")\n",
    "    print(f\"TXT → {out_txt}\")\n",
    "\n",
    "    print(\"\\nPer-book stats (first 6):\")\n",
    "    for s in per_book_stats[:6]:\n",
    "        print(s)\n",
    "\n",
    "    for i in range(min(3, len(df))):\n",
    "        r = df.iloc[i]\n",
    "        print(f\"[{i+1}] [{r['book']} {r['verse']}] \"\n",
    "              f\"{l1_name}({r[f'{l1_name} ID']}): {r[f'{l1_name}']}  ||  \"\n",
    "              f\"{l2_name}({r[f'{l2_name} ID']}): {r[f'{l2_name}']}\")\n",
    "\n",
    "    return out_csv, out_txt, df\n",
    "\n",
    "results = []\n",
    "for pair in PAIRS:\n",
    "    csv_path, txt_path, df_out = merge_pair_by_verse_dirs(\n",
    "        pair[\"lang1_dir\"], pair[\"lang2_dir\"],\n",
    "        out_base_name=pair[\"out_txt\"],\n",
    "        out_dir=OUT_DIR,\n",
    "        use_fixed_headers=USE_FIXED_HEADERS,\n",
    "        keep_empty=KEEP_EMPTY_LINES,\n",
    "    )\n",
    "    results.append((csv_path, txt_path, len(df_out)))\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Summary:\")\n",
    "for (csv_path, txt_path, nrows) in results:\n",
    "    print(f\"  {os.path.basename(csv_path)} | rows={nrows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
